
%----------MLP------------------------------
\subsubsection{Multi-Layer Perceptron}

7
A standard MLP is called with the \texttt{keras\_model\_sequential()}
function. With the below syntax, a network architecture can be defined,
model compiled and optimization schemes defined, and fit accordingly to
the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}
\NormalTok{neuralnetwork }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()           }\CommentTok{\# Define architecture}
\NormalTok{      neuralnetwork }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{6}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{, }\AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{21}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{4}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}softmax\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
          \FunctionTok{compile}\NormalTok{(}\AttributeTok{loss =} \StringTok{\textquotesingle{}mean\_squared\_error\textquotesingle{}}\NormalTok{,      }\CommentTok{\# Compile model}
          \AttributeTok{optimizer =} \StringTok{\textquotesingle{}sgd\textquotesingle{}}\NormalTok{,}
          \AttributeTok{metrics =} \StringTok{\textquotesingle{}accuracy\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
              \FunctionTok{fit}\NormalTok{(training,                         }\CommentTok{\# Train model}
\NormalTok{              trainLabels,}
              \AttributeTok{epochs =} \DecValTok{200}\NormalTok{,}
              \AttributeTok{batch\_size =} \DecValTok{32}\NormalTok{,}
              \AttributeTok{validation\_split =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When defining network architecture, each layer can be added and their
size and activation function specified. \texttt{layer\_dense} indicates
a fully-connected (dense) layer, meaning connections exist between all
neurons of all subsequent layers. In the compilation stage, a loss
function and optimization algorithm can be defined. The \texttt{metrics}
argument rates the model performance. Lastly fitting a model involves
commands that tell \texttt{keras} how to train and aggregate
simultaneous models to generate a useful network.



%----------CNN------------------------------
\subsubsection{Convolutional Neural Networks}

In \texttt{keras}, the following code defines a two-dimensional network
for computer vision tasks in a CNN. Because it is two-dimensional, it is
restricted to grayscale images. For color images, a three-dimesional
network is needed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Model architecture}
\NormalTok{CNN }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()}
\NormalTok{       CNN }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{32}\NormalTok{,}
                    \AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{,}
                    \AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{,}\DecValTok{28}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{64}\NormalTok{,}
                    \AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}\AttributeTok{pool\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{10}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}softmax\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This network analyses grayscale images of 28x28 pixels. The first output
convolutional layer size is calculated by subtracting the kernel size
from the pixel size and adding 1: it is a [28-3+1,28-3+1] = 
26x26 pixel image with 32 filters (defined above). Notice it is of
reduced height and width, but with increased depth. The second
convolutional layer size is calculated by a re-iteration of the previous
formula with the updated dimension: [26-3+1,26-3+1] =  24x24.
Note that CNN layers are not densely connected as they are for MLP's.
Because the data is so fine-grained (i.e.~one data point with a
relatively low pixel value has \(28 \cdot 28 = 784\) parameters),
loosening the layer connection makes for more efficient processing.
All-in-all, this network has 609,354 estimable parameters among its
layers. Had there existed a dense connection in convolutional layers,
the number of parameters would have exceeded 40 million.




%----------RNN------------------------------

\subsubsection{Recurrent Neural Networks
(RNN)}\label{recurrent-neural-networks-rnn}

The \texttt{keras} model for RNN architecture is defined below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Model architecture}
\NormalTok{RNN }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()}
\NormalTok{        RNN }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =} \DecValTok{1000}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_simple\_rnn}\NormalTok{(}\AttributeTok{units =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The embedding layer of the RNN allows for ease of computation by
translating words into vectorized densities of words in vocabulary.
\texttt{input\_dim} specifies the vector of values able to be put into
the network (1000 words) and \texttt{output\_dim} defines the vector
length. The RNN layer \texttt{layer\_simple\_rnn} defines a
fully-connected RNN where the output is intended to be returned to the
input. Lastly, the density layer delivers the final result. Above, it is
set to one unit (binary outcome). Increasing the number of units in this
layer would result in more classifiers for the output based on a one-hot
encoded result from the network.