
\section{Types of Neural Networks} %-------------SECTION

The previous section described a Multi-Layer Perceptron network. This
section is devoted to other network types and their most practical uses. Listed are a few primary network types and their general purposes.

At the cutting edge, neural networks can be built with the
\texttt{keras} package in \texttt{R}. Keras is a high-level API centered
on the construction of deep learning models in a fast, user-friendly
environment. It uses the \texttt{TensorFlow} engine built by Google and
is written in \texttt{Python}. \texttt{keras} is fully integrated with
\texttt{R}, though an installation of \texttt{Python} is required to run
the package. Features of the \texttt{keras} package will be discussed in
conjunction with the type of model described.

A standard MLP is called with the \texttt{keras\_model\_sequential()}
function. With the below syntax, a network architecture can be defined,
model compiled and optimization schemes defined, and fit accordingly to
the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}
\NormalTok{neuralnetwork }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()           }\CommentTok{\# Define architecture}
\NormalTok{      neuralnetwork }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{6}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{, }\AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{21}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{4}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}softmax\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
          \FunctionTok{compile}\NormalTok{(}\AttributeTok{loss =} \StringTok{\textquotesingle{}mean\_squared\_error\textquotesingle{}}\NormalTok{,      }\CommentTok{\# Compile model}
          \AttributeTok{optimizer =} \StringTok{\textquotesingle{}sgd\textquotesingle{}}\NormalTok{,}
          \AttributeTok{metrics =} \StringTok{\textquotesingle{}accuracy\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
              \FunctionTok{fit}\NormalTok{(training,                         }\CommentTok{\# Train model}
\NormalTok{              trainLabels,}
              \AttributeTok{epochs =} \DecValTok{200}\NormalTok{,}
              \AttributeTok{batch\_size =} \DecValTok{32}\NormalTok{,}
              \AttributeTok{validation\_split =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When defining network architecture, each layer can be added and their
size and activation function specified. \texttt{layer\_dense} indicates
a fully-connected (dense) layer, meaning connections exist between all
neurons of all subsequent layers. In the compilation stage, a loss
function and optimization algorithm can be defined. The \texttt{metrics}
argument rates the model performance. Lastly fitting a model involves
commands that tell \texttt{keras} how to train and aggregate
simultaneous models to generate a useful network.

\hypertarget{convolutional-neural-networks-cnn}{%
\subsection{Convolutional Neural Networks
(CNN)}\label{convolutional-neural-networks-cnn}}

A Convolutional Neural Network (CNN) is a neural network that uses the \textbf{convolution} operation in at least one of its layers.  The convolution operation is as follows:

Convolution operation continuous \[
C(t) = \int x(\tau)w(t - \tau)d\tau
\] Convolution operation discrete \[
C(t) = \sum_{\tau = -\infty}^\infty x(\tau)w(t - \tau)
\] \(x(t)\) is the \textbf{input} and \(w(t-\tau)\) is known as the
\textbf{kernel}. The shorthand operation for the convolution operation
is denoted with an asterisk: \[
(x * w)(t)
\]

This is primarily useful for computer vision models.  The kernel, which is much smaller than the image, moves across the image and performs its calculation on each pixel value within the proportion of the image it has captured.  It moves throughout the image and calculates on every combination of adjacent pixels until all pixels are accounted for.

Assumptions for CNN's \cite{Goodfellow-et-al-2016}
\begin{itemize}
\tightlist
\item{output is essentially a weighted average}
\item{\(w\) must be a valid probability distribution and 0 for all negative arguments}
\item{values of kernel and input tensors are zero except for the finite set in which values are stored (i.e. the size of the image)}
\item{infinite summation over finite number of array elements}
\end{itemize}

Two-dimensional image \(X\) with a two-dimensional kernel \(W\) \[
C(i,j) = (X * W)(i,j) = \sum_m \sum_n X(m,n)W(i-m,j-n)
\]

\(m\) and \(n\) are the representative pixel locations in the relevant
dimension, while \(i\) and \(j\) are the kernel locations, where the
convolution is taking place. The infinite summation is made possible by
the fact that values are zero wherever the image is not present.

In \texttt{keras}, the following code defines a two-dimensional network
for computer vision tasks in a CNN. Because it is two-dimensional, it is
restricted to grayscale images. For color images, a three-dimesional
network is needed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Model architecture}
\NormalTok{CNN }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()}
\NormalTok{       CNN }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{32}\NormalTok{,}
                    \AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{,}
                    \AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{,}\DecValTok{28}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{64}\NormalTok{,}
                    \AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}\AttributeTok{pool\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{10}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}softmax\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This network analyses grayscale images of 28x28 pixels. The first output
convolutional layer size is calculated by subtracting the kernel size
from the pixel size and adding 1: it is a [28-3+1,28-3+1] = 
26x26 pixel image with 32 filters (defined above). Notice it is of
reduced height and width, but with increased depth. The second
convolutional layer size is calculated by a re-iteration of the previous
formula with the updated dimension: [26-3+1,26-3+1] =  24x24.
Note that CNN layers are not densely connected as they are for MLP's.
Because the data is so fine-grained (i.e.~one data point with a
relatively low pixel value has \(28 \cdot 28 = 784\) parameters),
loosening the layer connection makes for more efficient processing.
All-in-all, this network has 609,354 estimable parameters among its
layers. Had there existed a dense connection in convolutional layers,
the number of parameters would have exceeded 40 million.

\hypertarget{recurrent-neural-networks-rnn}{%
\subsection{Recurrent Neural Networks
(RNN)}\label{recurrent-neural-networks-rnn}}

Connections between nodes of a RNN can upcycle, allowing for the output of
one neuron to be input into a neuron before it.
\cite{medsker2001recurrent}  This is especially useful for sequential data, such as language/text classification and time series forecasting. Mappings of connections between outputs can be \emph{one-to-one},
\emph{one-to-many}, \emph{many-to-one}, or \emph{many-to-many}.

The \texttt{keras} model for RNN architecture is defined below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Model architecture}
\NormalTok{RNN }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()}
\NormalTok{        RNN }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =} \DecValTok{1000}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_simple\_rnn}\NormalTok{(}\AttributeTok{units =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The embedding layer of the RNN allows for ease of computation by
translating words into vectorized densities of words in vocabulary.
\texttt{input\_dim} specifies the vector of values able to be put into
the network (1000 words) and \texttt{output\_dim} defines the vector
length. The RNN layer \texttt{layer\_simple\_rnn} defines a
fully-connected RNN where the output is intended to be returned to the
input. Lastly, the density layer delivers the final result. Above, it is
set to one unit (binary outcome). Increasing the number of units in this
layer would result in more classifiers for the output based on a one-hot
encoded result from the network.

\hypertarget{generative-adversarial-networks-gan}{%
\subsection{Generative Adversarial Networks
(GAN)}\label{generative-adversarial-networks-gan}}

A unique element of these networks is that they are actually comprised of two networks - a \textit{generator} and a \textit{discriminator} - to compete against one another and generate new data.  This is typically applied in image generation applications.  The generator creates random data to send to the discriminator, which is designed to differentiate between real training values (i.e. labeled images) and the fake generated ones.  These two networks interact until the generator starts to produce realistic data that the discriminator cannot easily differentiate from the training data.  This network type deviates from the other common types above in that it has a source of random variation in its training.  An extensive tutorial to all above mentioned network types and more to build in \texttt{R} with \texttt{keras} is available in (Rai, 2019 \cite{rai}).