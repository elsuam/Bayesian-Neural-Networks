% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Keras Models},
  pdfauthor={Samuel Richards},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Keras Models}
\author{Samuel Richards}
\date{}

\begin{document}
\maketitle

The previous section described a Multi-Layer Perceptron network. This
section is devoted to other network types and their most practical uses.
Listed are a few essential network types; it is not exhaustive of every
model common to deep learning application

At the cutting edge, neural networks can be built with the
\texttt{keras} package in \texttt{R}. Keras is a high-level API centered
on the construction of deep learning models in a fast, user-friendly
environment. It uses the \texttt{TensorFlow} engine built by Google and
is written in \texttt{Python}. \texttt{keras} is fully integrated with
\texttt{R}, though an installation of \texttt{Python} is required to run
the package. Features of the \texttt{keras} package will be discussed in
conjunction with the type of model described. This will either
supplement or replace the neural networks described

A standard MLP is called with the \texttt{keras\_model\_sequential()}
function. With the below syntax, a network architecture can be defined,
model compiled and optimization schemes defined, and fit accordingly to
the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}
\NormalTok{neuralnetwork }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()           }\CommentTok{\# Define architecture}
\NormalTok{      neuralnetwork }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{6}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{, }\AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{21}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{4}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}softmax\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
          \FunctionTok{compile}\NormalTok{(}\AttributeTok{loss =} \StringTok{\textquotesingle{}mean\_squared\_error\textquotesingle{}}\NormalTok{,      }\CommentTok{\# Compile model}
          \AttributeTok{optimizer =} \StringTok{\textquotesingle{}sgd\textquotesingle{}}\NormalTok{,}
          \AttributeTok{metrics =} \StringTok{\textquotesingle{}accuracy\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
              \FunctionTok{fit}\NormalTok{(training,                         }\CommentTok{\# Train model}
\NormalTok{              trainLabels,}
              \AttributeTok{epochs =} \DecValTok{200}\NormalTok{,}
              \AttributeTok{batch\_size =} \DecValTok{32}\NormalTok{,}
              \AttributeTok{validation\_split =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

When defining network architecture, each layer can be added and their
size and activation function specified. \texttt{layer\_dense} indicates
a fully-connected (dense) layer, meaning connections exist between all
neurons of all subsequent layers. In the compilation stage, a loss
function and optimization algorithm can be defined. The \texttt{metrics}
argument rates the model performance. Lastly fitting a model involves
commands that tell \texttt{keras} how to train and aggregate
simultaneous models to generate a useful network.

\hypertarget{convolutional-neural-networks-cnn}{%
\subsection{Convolutional Neural Networks
(CNN)}\label{convolutional-neural-networks-cnn}}

A Convolutional Neural Network (CNN) is a neural network that uses the
convolution operation in at least one of its layers \cite{?}.

Convolution operation continuous \[
C(t) = \int x(\tau)w(t - \tau)d\tau
\] Convolution operation discrete \[
C(t) = \sum_{\tau = -\infty}^\infty x(\tau)w(t - \tau)
\] \(x(t)\) is the \textbf{input} and \(w(t-\tau)\) is known as the
\textbf{kernel}. The shorthand operation for the convolution operation
is denoted with an asterisk: \[
(x * w)(t)
\]

Assumptions for CNN's \cite{Goodfellow-et-al-2016} - output is
essentially a weighted average - \(w\) must be a valid probability
distribution and 0 for all negative arguments - values of kernel and
input tensors are zero except for the finite set in which values are
stored (i.e.~size of image) - infinite summation over finite number of
array elements

Two-dimensional image \(X\) with a two-dimensional kernel \(W\) \[
C(i,j) = (X * W)(i,j) = \sum_m \sum_n X(m,n)W(i-m,j-n)
\]

\(m\) and \(n\) are the representative pixel locations in the relevant
dimension, while \(i\) and \(j\) are the kernel locations, where the
convolution is taking place. The infinite summation is made possible by
the fact that values are zero wherever the image is not present.

In \texttt{keras}, the following code defines a two-dimensional network
for computer vision tasks in a CNN. Because it is two-dimensional, it is
restricted to grayscale images. For color images, a three-dimesional
network is needed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Model architecture}
\NormalTok{CNN }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()}
\NormalTok{       CNN }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{32}\NormalTok{,}
                    \AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{,}
                    \AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{,}\DecValTok{28}\NormalTok{,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{64}\NormalTok{,}
                    \AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                    \AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}\AttributeTok{pool\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}relu\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
       \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{10}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}softmax\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This network analyses grayscale images of 28x28 pixels. The first output
convolutional layer size is calculated by subtracting the kernel size
from the pixel size and adding 1: it is a \${[}28-3+1,28-3+1{]} = \$
26x26 pixel image with 32 filters (defined above). Notice it is of
reduced height and width, but with increased depth. The second
convolutional layer size is calculated by a re-iteration of the previous
formula with the updated dimension: \${[}26-3+1,26-3+1{]} = \$ 24x24.
Note that CNN layers are not densely connected as they are for MLP's.
Because the data is so fine-grained (i.e.~one data point with a
relatively low pixel value has \(28 \cdot 28 = 784\) parameters),
loosening the layer connection makes for more efficient processing.
All-in-all, this network has 609,354 estimable parameters among its
layers. Had there existed a dense connection in convolutional layers,
the number of parameters would have exceeded 40 million.

\hypertarget{recurrent-neural-networks-rnn}{%
\subsection{Recurrent Neural Networks
(RNN)}\label{recurrent-neural-networks-rnn}}

Connections between nodes of a RNN can cycle, allowing for the output of
one neuron to be input into a neuron before it.
\cite{medsker2001recurrent} This is especially useful for sequential
data, such as language/text classification an time series forecasting.
Mappings of connections between outputs can be \emph{one-to-one},
\emph{one-to-many}, \emph{many-to-one}, or \emph{many-to-many}.

The \texttt{keras} model for RNN architecture is defined below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Model architecture}
\NormalTok{RNN }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{()}
\NormalTok{        RNN }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =} \DecValTok{1000}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_simple\_rnn}\NormalTok{(}\AttributeTok{units =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The embedding layer of the RNN allows for ease of computation by
translating words into vectorized densities of words in vocabulary.
\texttt{input\_dim} specifies the vector of values able to be put into
the network (1000 words) and \texttt{output\_dim} defines the vector
length. The RNN layer \texttt{layer\_simple\_rnn} defines a
fully-connected RNN where the output is intended to be returned to the
input. Lastly, the density layer delivers the final result. Above, it is
set to one unit (binary outcome). Increasing the number of units in this
layer would result in more classifiers for the output based on a one-hot
encoded result from the network.

\hypertarget{generative-adversarial-networks-gan}{%
\subsection{Generative Adversarial Networks
(GAN)}\label{generative-adversarial-networks-gan}}

Image generation to try and fool a ``discriminator'' network

\hypertarget{long-short-term-memory-network}{%
\subsection{Long Short-Term Memory
Network}\label{long-short-term-memory-network}}

Special type of RNN \cite{Rai}

\end{document}
