\chapter{Bayesian Neural Networks}

 In the Bayesian framework, training a neural network to minimize its error based on a single point estimate $w$ is equivalent to maximizing the likelihood of the training data (i.e. finding weight w that maximizes $P(D_{train}|w)$ \cite{bishop1995} Therefore we can use Bayesâ€™ Rule to compute an interval of potential weights based on probabilities $P(w|D_{train})$

\section{Architecture}

Stochastic neural networks, which use either stochastic activations or stochastic weights, simulate multiple possible models $\theta$ with associated probability distribution $p(\theta)$.  Aggregating independent predictors can lead to better predictions than single point estimates. \cite{Jospin}.  In comparing the predictions of multiple sample $\theta$, stochastic models better measure uncertainty.  Bayesian neural networks are a special case of stochastic neural networks in which the ensemble of possible models is obtained using Bayesian inference. \cite{mackay1992practical}

\subsection{Inference}

Applying Bayes Theorem to training inputs:
$$
p(\theta|D) = \frac{p(D_{y}|D_{x},\theta)p(\theta)}{\int_\theta p(D_{y}|D_{x},\theta')p(\theta')d\theta'}
$$
Where $D$ is the observed training data and $\theta$ represents the distribution of model parameters (weights and biases).


The marginal is of particular interest to integrate out unwanted parameters.  (keep only outputs $Y$ and distributions $\Theta$)
(write about marginalization in Bayes section)

$$
Equation (5) in Jospin
$$


\section{Model Performance Metrics}

Comparison how assessing and improving the performance of a BNN compares to that of ANN's.






\subsection{Description of Uncertainty}

BNN uncertainty \cite{Jospin}

Bayes comes complete with uncertainty measures already configured by the posterior distribution.  This is a standard feature, straight out of the box, no assembly required.


Model $p(y|\theta)$ is a function of y that describes the aleatoric uncertainty given fixed $\theta$ (random variation).
Likelihood $p(y|\theta)$ is a function of $\theta$ that helps infer epistemic uncertainty given observed data $y$ (the model's uncertainty in its outcome, as seen earlier).


For regression and classification, average predictions of a BNN to generate result
$$
Equation (6) / (8) hybrid
$$

When performing regression, uncertainty is computed by the covariance matrix:
$$
Equation (7) here
$$

For classification tasks, uncertainty is measured by the relative probability of each class.
$$
Equation (8) and (9) here
$$

\section{Fitting a BNN}

Final run at the Tohoku Earthquake example that fits an \textit{ACTUAL} BNN with plenty of performance metrics displayed.


Each Subsection will be the main steps as shown in my R code.