---
title: "Thesis Figures and Code Snippets"
subtitle: "STA 610"
author: "Samuel Richards"
output:
  pdf_document:
    keep_tex: TRUE
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: no
---

<!--Use this file for a separate project; modify it for Thesis when it is in Overleaf -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = 'Figures/')
library(tidyverse)
library(gridExtra)
```

# Artificial Neural Networks

## Step Function
```{r step-function, echo=FALSE}
step <- function(x){
  ifelse (x <= 0,0,1) 
}

f <- seq(-8,8, length=1000)

plot(f,step(f), type = 's', lwd = 8, main="Step Function", xlab = " ", ylab = " ", ylim = c(0,1), col = "turquoise", frame.plot = T)
```


## Sigmid Function
```{r sigmoid-function, echo=FALSE}
sigmoid <- function(x){
  return(1 / (1 + exp(-x)))
}

f <- seq(-8,8, length=1000)

plot(f,sigmoid(f), main="Sigmoid Function", xlab = " ", ylab = " ", ylim = c(0,1), col = "turquoise", frame.plot = T)
```


## Other Activation Functions
```{r other-activation-functions, echo=FALSE}
relu <- function(x){
  ifelse (x<0,0,x) 
}

f <- seq(-8,8, length=1000)

plot(f,tanh(f), main="Hyperbolic Tangent", xlab = " ", ylab = " ", col = "turquoise")
plot(f,relu(f), main="ReLU", xlab = " ", ylab = " ", col = "turquoise")
```



## Gradient Descent

### 2-D example
```{r grad_desc_2D, echo=FALSE}
f <- seq(-8,8, length=1000)

plot(f,2*f^3+10*f^2, main=" ", xlab = " ", ylab = " ", col = "darkred")
```

### 3-D example
```{r grad_desc, eval=FALSE, include=FALSE}
#Only use the final output from the 2D animation and the 3D output of the relevant function
# turned off so not to overflow figures folder

library(animation) 

ani.options(interval = 0.8, nmax = 50)
f1 = function(x, y) 1/2*x^2 + 4*sin(y)

xx = grad.desc(f1, pi * c(-2, -2, 2, 2), c(-2 * pi, 2))

xx$persp(col = "lightblue", theta = 30, phi = 30)
```


## Regularization

### Overfit and Underfit

```{r}
#Generate some noisy data
set.seed(3)
a <- runif(12, min=0, max=90)
c <- 24*sin(.05*a) + rnorm(length(a),0,5) + 40

#create data frame and base plot
df <- data.frame(a,c)
model <- ggplot(df, aes(x = a, y = c)) +
        geom_point(size = 3, color = "purple") +
        ylim(30,80) +
        theme_minimal()


# underfit model

model +
    stat_smooth(method = lm,
              formula = y ~ poly(x,1),
              fullrange = T,
              color = "turquoise",
              se = F)

# overfit model

pol <- 9 # polynomial order
model + 
    stat_smooth(method = lm,
              formula = y ~ poly(x,pol),
              fullrange = T,
              color = "turquoise",
              se = F)


```


<!---
foiled attempt at replicating the graph from Goodfellow
```{r eval=FALSE, include=FALSE}

polynomials <- 1:11

MSE <- NULL
testerr <- NULL
for (i in polynomials) {
    mod_training <- lm(c ~ poly(a, degree = i), data = df)
    predictions <- predict(mod_training, data = df)
    MSE[i] <- mean((df$c - predictions)^2)
    testerr[i] <- abs(predictions - c[i])
}

pd <- data.frame(polynomials, MSE, testerr)

ggplot(pd, aes(x = polynomials, y = MSE)) +
    geom_line() +
    geom_line(y = testerr) +
    scale_x_continuous(breaks = polynomials) +
    theme_bw()
```
-->



# Bayes


## Bayesian Updating

For each possible explaination of the sample, count all the ways in which the sample could happen. Explanations with more ways to produce the sample are more plausible.

```{r}
prob <- runif(1,0,1) #single observation from a uniform distribution to be the probability we want to find
#prob <- .6
samplesize <- 8
df <- NULL


#Beta Distribution: alpha-1 successes and beta-1 failures

for (i in 1:samplesize){
  good <- seq(0,1, length = 101)  #Denotes a good observation
  bad <- seq(1,0, length = 101)   #Denotes a bad observation
  #  if(i %% 10==0){                    #for every 10th sample (when activated and sample size is increased)
    sample <- rbinom(i,1, prob)         #random draw of 0 or 1 i times
    y <- good^(sum(sample==1))*bad^(sum(sample==0)) #observations of 'good' and 'bad' exponentiated respective to                                                       binomial draws (1 for good, 0 for bad)
    dat <- data.frame(x=seq(0,1, length = 101), y=y/max(y), col=rep(i:i, each=101)) #normalize y and place values into a data frame
    df <- rbind(df,dat)                             #repeat for each value of i into same data frame
   # }
}

#This is just a shape of the Beta Distribution with parameters set at random, based on probability from the first line. It does not multiply by likelihood and normalize.  Up next, snag some code from the chunks below to try and get that posterior moving.

 # plot data
ggplot(df,aes(x=x,y=y,group=col,colour=factor(col), alpha = col )) +
  geom_line(size = 1.5)

 # plot most recent posterior only
ggplot(df[which(df$col == samplesize),], aes(x=x,y=y,group=col,colour=factor(col) )) + 
  geom_line(size = 1.5)
```



```{r}
p_grid <- seq(from=0, to=1, length.out = 1000)  #1000 values for proportion (p) between 0 and 1

prob_p <- dbeta(p_grid, 3,10) #prior distribution (interval of previous assumptions/explanations (p) and the probability for each value)
#(note that you do not need data for a prior distribution - the data washes out the "falsities" of the prior)

prob_data <- dbinom(4, size=50, prob=p_grid) #Relative number of ways we can see W out of (W+L) from the Binomial Sampling Formula for all of the respective proportions outlined in p_grid

posterior <- prob_data * prob_p #sets up and normalizes the posterior distribution
posterior <- posterior / sum(posterior)

# issues arise when plotting (values not on the same scale)

#comp <- data.frame(prob_p, posterior) #to compare prior and posterior values

# prior <- cbind(p_grid,prob_p, dens = "prior")
# posterior <- cbind(p_grid,posterior, dens = "posterior")
# 
# df <- as.data.frame(rbind(prior,posterior))
# 
# ggplot(df, aes(x = p_grid, y = prob_p, group = dens)) +
#   geom_line()

plot(prob_p)    #plots the prior distribution
plot(prob_data)
plot(posterior)
```



## BNN model capacity preference

```{r}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), linewidth = 1.5, aes(colour = "B_3")) + ylab("") +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = .6), linewidth = 1.5, aes(colour = "B_2")) + ylab("") +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = .35), linewidth = 1.5, aes(colour = "B_1")) + ylab("") +
 # scale_y_continuous(breaks = NULL) +
  scale_colour_manual("Lgend title", values = c("gray60", "gray50", "gray40")) +
  geom_vline(xintercept = .6,
             color = "darkred",
             size=1.5,
             alpha = .8) +
  theme_minimal()
```


















<!-- old Bayesian Updating code that uses base R plot function
```{r eval=FALSE, include=FALSE}
#ways/sum(ways)

prob <- runif(1,0,1) #single observation from a uniform distribution to be the probability we want to find


sample <- rbinom(100,1, prob)
#plot(sample)

W <- sum(sample=="1")
L <- sum(sample=="0")
#p <- seq(0,1,length = 100)


p_grid <- seq(from=0, to=1, length.out = 100)  #100 values for proportion (p) between 0 and 1

#prior distribution 
prior <- dbeta(p_grid, 10,10) #(interval of previous assumptions/explanations (p) and the probability for each value)

#(note that you do not need data for a prior distribution - the data washes out the "falsities" of the prior)

likelihood <- dbinom(sum(W), size=100, prob=p_grid) #Relative number of ways we can see W out of (W+L) from the Binomial Sampling Formula for all of the respective proportions outlined in p_grid

posterior <- likelihood * prior #sets up and normalizes the posterior distribution
posterior <- posterior / sum(posterior)


plot(prior)

plot(likelihood)

plot(posterior)    #plots the posterior distribution
```
-->














