\chapter{Bayesian Statistical Methods}

Often called inverse probability in its budding stages, \cite{salsburg2002lady} Bayesian Statistics is an approach to analyzing data by use a theorem developed by the Reverend Thomas Bayes, although the theorem was not published until after his death.  Bayes' Theorem (or Bayes' Rule) describes the conditional probability of an event.

The frequentist approach describes probability as the relative frequency of a favorable outcome as the number of samples increases to infinity.  In Bayes, probability is the number of favorable outcomes divided by the total number of possible outcomes \cite{gelmanbayesian3}
$$
P = \frac{N_{\text{favorable outcomes}}}{N_{\text{total possible outcomes}}}
$$

It is important to note that to apply statistical methods by use of Bayes does not violate any accords of frequentism and that it does not stand as an opposition to the frequentist analogue.  Bayesian methods simply offer another perspective, with some useful features in specialized cases - one of which is the ability to generate mathematically savvy results with very little data.  The downside is that to find that "total number of possible outcomes" can be very computationally intensive.


\section{Fundamentals of Bayesian Inference} %-------------SECTION

%\textit{(theoretical representations)}

A few things change when operating with the Bayesian framework.  For one, the goal is not to construct a model to estimate parameters as fixed values.  Rather, parameters are treated as random variables themselves, with the same properties as any other random variable - each could have a mean, a variance, minimum maximum, etc., and the distribution of that parameter reveals that information.

Whereas frequentism often employes maximum likelihood modeling $p(y|\theta)$ to find the parameters $\theta$ which give the most likely estimation of the data, a Bayesian probabilistic approach finds the most likely distribution of parameters from the data $p(\theta|D)$\cite{mullachery2018bayesian}.  Bayesian inference (by Bayes' Theorem, given data set $D$) is as follows:

$$
p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)} = \frac{p(D|\theta)p(\theta)}{\int_{\theta'} p(D|\theta')p(\theta')d\theta'}
$$


\subsection{From Prior to Posterior}

To determine the posterior $p(\theta|D)$ requires selection of a prior $p(\theta)$ and determination of the likelihood of the data under supposition of $\theta$, $p(D|\theta)$.  The denominator is often regarded as the \textit{normalizing constant}, which ensures the outcome is a probability between 0 and 1.  Without it, the posterior is represented as being proportional to the prior and the likelihood $p(\theta|D) \propto p(D|\theta)p(\theta)$, but not interpretable as a posterior probability.  The issue tends to be in that the normalizing constant is a hardship to calculate exactly and often impossible at all.  This is where estimation techniques come in, but more on that later.
%Selection of priors, perhaps advise on broad priors that tell little information.
Typically, priors within the same family as the likelihood (i.e. a Bernoulli likelohood and a Beta prior) have analytically helpful properties \cite{mullachery2018bayesian}. These \textit{conjugate priors}, while mathematically useful, often place undesirable assumptions on the model.  Therefore it is usually preferred not to rely on conjugate priors in order to sacrifice mathematical precision for model accuracy by approximation.
%prior to posterior involves using MLE and the prior as we have done before in 506  Use my notes from then to compute a posterior distribution for a Gaussian prior

\textbf{Bayesian Updating} is a concept in which the posterior distribution can be reused as the prior distribution when more data arrives \cite{mcelreath2016statistical}.  This is a useful attribute because the original prior outlines some assumptions of a model, which are then used to compute the posterior, the conditional probability of the parameters in light of those assumptions and the data likelihood.  By holding on to the posterior, the model can predict more realistic expectations based on the previous assumptions and the pattern of data that the model has already seen before.

%\subsection{Bayesian Updating}




\section{Practical Computing} %-------------SECTION

 As mentioned, the premier disadvantage of Bayesian inference is that is requires computationally intractable integrations.  In response, a world of practical approximation techniques exists, some of which will be discussed in this section.  These approximaton techniques can be shown to still have pristine results while dodging tricky or impossible integrals \cite{tipping2004bayesian}.

 Approximate Bayesian inference

$$
p(w|D) \approx \frac{p(D|w)p(w)}{Z}
$$
In which $Z$ is approximated by some sophisticated means.

 %mention other approximation techniques from Rethinking like Laplace and Grid (lecture 8 notes and referenced book pages) and lead into \textbf{Markov Chain Monte Carlo Methods}

Simpler models can be estimated by means of grid approximation \cite{mcelreath2016statistical}, in which the domain of the posterior is split up into many finite intervals and values calculated accordingly.  Quadratic Approximation is another method for approximating the posterior.  It assumes the area by the peak of the posterior distribution is approximately normally distributed, which is common \cite{mcelreath2016statistical}, and approximates that distribution because the logarithm of a normal distribution forms a quadratic function.  Both of these techniques pail in comparison to use of \textbf{Markov Chain Monte Carlo} Methods.  In essence, a Markov Chain is one which the $i^{th}$ outcome is dependent upon the $(i-1)^{th}$ outcome.  MCMC is a class of algorithms based on this feature.  Some algorithms below are described for their techniques to estimating very closely a complex posterior distribution.  Unless otherwise cited, most descriptions come from (McElreath, 2016 \cite{mcelreath2016statistical}) and (Gelman et. al, 2021 \cite{gelmanbayesian3}) 


%\textit{(Relevant examples in STAN whenever appropriate)}

\subsection{Metropolis (Rosenbluth-Teller) Algorithm}

The Metropolis Algorithm is named after the first author of the published article that introduced the world to their "Fast Computing Machines" \cite{metropolis1953equation}, although it is unclear how the work was divided. Marshall Rosenbluth and Edward Teller are co-authors, along with their wives, who performed much of the calculations.  The algorithm is as follows:

\begin{enumerate}
\tightlist
\item{Set current point $\theta_0$ with probability $p(\theta_0|y) > 0$}

\item{Sample a proposal $\theta^*$ from a proposal distribution $J_t(\theta^* | \theta^{t-1})$ at time $t$.}

\item{The acceptence of the proposal is $A = min \left( \frac{p(\theta^*|y)}{p(\theta^{t-1})|y} , 1 \right) $}
\end{enumerate}

This means that the proposed move will always be accepted if it is toward a higher probability position that current.  Otherwise, it will be accepted with probability $\frac{p(\theta*|y)}{p(\theta^{t-1})|y}$

Works when the proposal distribution is symmetric, that is, when $J_t(\theta^* | \theta^{t-1}) = J_t(\theta^{t-1} | \theta^*)$

\subsection{Metropolis-Hastings Algorithm}

To allow for non-symmetric distributions, the Metropolis-Hastings algorithm generalizes the Metropolis algorithm.  The ratio of densities becomes:
$$
\frac{ p(\theta^*|y) / J_t(\theta^* | \theta^{t-1}) }{ p(\theta^{t-1}|y) / J_t(\theta^{t-1} | \theta^*) }
$$

and a minimum between that ratio and 1 is selected.

\subsection{Hamiltonian Monte Carlo}

Rather than a random walk through the posterior, HMC moves the exploration problem into a space containing a position parameter $\lambda$ and a momentum parameter $r$.  HMC has the ability to make confident, far stride and as a result converge faster than other algorithms \cite{mullachery2018bayesian}.
The parameters of HMS are step size $s$ and number of steps $L$. Sampling from a standard multivariate normal distribution, Leapfrog updates are made to determine the proposed position $\tilde{lambda}$ and momentum $\tilde{r}$.  The proposed values are determined, through use of Laplace transform $\mathcal{L}$, by
$$
min\left( 1, \frac{exp[\mathcal{L}(\tilde{\lambda} - \frac{1}{2} \tilde{r} \cdot \tilde{r}]}
{exp[\mathcal{L}(\lambda^{m-1}) - \frac{1}{2} r^0 \cdot r^0] }
\right)
$$
and, if accepted, the new position is $\lambda^m \leftarrow \tilde{\lambda}$ and momentum $r^m \leftarrow -\tilde{r}$

\subsection{Gibbs Sampling}

Gibbs Sampling is another variant of M-H that uses clever proposals based on the criterea of alternating conditional sampling

Two-variable example: \cite{geman1984stochastic}  

$p(x,y)$ is a probability density that can be sampled more easily as conditional distributions $p(x|y)$ and $p(y|x)$.  Gibbs Samping is as follows:
\begin{enumerate}
\item{initialize $(x_0,y_0)$}
\item{sample $x_1 \sim p(x|y_0)$, then $y_1 \sim p(y|x_1)$, then $x_2 \sim p(x|y_1)$, then $y_2 \sim p(y|x_2)$}
\item{ ...}
\item{sample $x_n \sim p(x|y_{n-1})$, then $y_n \sim p(y|x_n)$}
\end{enumerate}

In higher dimensions:

\begin{itemize}
\tightlist
\item{$\theta_j$ represents the subvector that makes up the components of $\theta$.  For example, a parameter representing categories "blue", "red", "green", and "yellow" has four components.}
\item{At each iteration $t$, each level of $\theta^t_j$ is sampled ($\theta^t_{blue}$, $\theta^t_{red}$, $\theta^t_{green}$, $\theta^t_{yellow}$)}
\item{ $p(\theta_j|\theta^{t-1}_{-j},y)$, where $\theta^{t-1}_{-j}$ represents all components of $\theta$ except for $\theta_j$.}
\end{itemize}



\subsection{Variational Inference}

MCMC methods are the bets methods for approximating the posterior with samples from the exact posterior, but their Achilles' heel lies in their lack of scalability \cite{Jospin}.  Variational inference methods convert an integration problem into an optimization problem \cite{mullachery2018bayesian}.  They introduce a distribution $q(\theta;H)$ called the variational distribution parameterized by $H$.  $q(\theta;H)$ must be flexible enough to provide a wide variety of posterior distributions in order to capture the posterior $p(\theta|D)$.  Closeness between the variational distribution and the true posterior is measured by Kullback-Leibler divergence \cite{blei2017variational}:
$$
KL(q||p) = E_q\left[ log\frac{q(\theta;H)}{p(\theta|D)} \right]
$$
The family of parameters $H$ is optimized to minimize $KL(q||p)$, in a similar manner that gradient descent seeks to optimize network parameters.  Much of variational inference has been omitted for this thesis, but extensive detail on these techniques can obtained from (Blai, 2017 \cite{blei2017variational}).


\section{Bayesian Data Analysis} %-------------SECTION

%\textit{(More practical representations)}

A nice feature of Bayesian Statistics is the ability to make sense out of few data points, without the need for minimum sample size or rules of thumb.  Another is the ability to extract the desired distribution of interest independent of the particular parameter settings. \cite{bishop2006pattern}

\subsection{Marginalization}

An essential element to Bayesian inference is \textit{marginalization}.  Rather than estimating all unnecessary variables, they are integrated out
\cite{tipping2004bayesian}.  Suppose a model is determined by use of Bayes.  Its frequentist counterpart may use Ordinary Least Squares to find a set of parameters $\theta$ which minimizes its error. In Bayes, the parameters $\theta$ can be expressed by the posterior distribution $p(\theta|y,\alpha,\sigma^2)$, with $\alpha$ derived from the prior and which assumes residual normailty with variation $\sigma^2$ (see Tipping, 2004 \cite{tipping2004bayesian} for derivation)

Integrate out unwanted parameters for the \textit{predictive distribution} of a new data $y$:
$$
p(y|x,\alpha,\sigma^2) = \int p(y|\theta,\sigma^2) p(\theta|x,\alpha,\sigma^2) dw
$$

Where  $p(y|x,\alpha,\sigma^2)$ is the posterior predictive distribution of predicted outputs $y$ and $p(y|\theta,\sigma^2)$ is the likelihood of the new data.  If $\theta$ and $\sigma^2$ were known to be true, then the likelihood would determine the prediction for future data.  Since these are unknown, and because under Bayes parameters are random variables, the predictive distribution is a weighted average over the posterior.  Thus, the  predictive distribution can be interpreted as the expectation of the single network likelihood under the posterior $p(y|x,\alpha,\sigma^2)$. \cite{salad} \cite{Jospin}.  
Setting aside the complex notation, this is simply a means of generating predictions from the parameter distribution gained by use of Bayes' Rule.  It describes the uncertainty the model has and allows for better flexibility without supplemental engineering.
In the absence of many data points, the predictive distribution will be very sparse. \cite{tipping2004bayesian}

%\textbf{Simulating the Predictive Distribution}
%$$
%Insert Simulationhere
%$$

Ideally, to be fully Bayesian and practice the mastery of marginalization is to integrate out \textit{all} variables that are not related to the task at hand. \cite{bishop2006pattern}.  The general Bayesian predictive framework employs the full posterior \cite{tipping2004bayesian}:
$$
p(\theta,\alpha,\sigma^2|x) = \frac{p(x|\theta,\sigma^2) p(\theta|\alpha)p(\alpha)p(\sigma^2)}{\iiint p(x|\theta,\sigma^2)p(\theta|\alpha)p(\alpha)p(\sigma^2) \text{ } d\theta \text{ } d\alpha \text{ } d\sigma^2}
$$
However, this is moving beyond the scope of this thesis.  Bayesian Statistics offers a multitude of possibilities to indulge statistical models beyond estimation, such as regularizing, determining thresholds like Information Criteria, cross validating, and more.  Libraries could be filled with the studies of its potential and relative simulations on complex machine learning models.  The next sections return to discuss neural networks with applications of Bayes to select certain features.

\begin{comment}
\subsection{Predictive Accuracy Measures}
I feel I need to put Bayes-CV somewhere...

Mention of their frequentist analog (for lppd is is the log-probability score on Rethinking p. 214)

\textbf{Bayesian Cross-Validation} to get the log-pointwise predictive density
$$
lppd_{CV} = \sum_{i=1}^N \frac{1}{S} \sum_{s=1}^S logP(w_i|\theta_{-i,s})
$$

\textbf{Information Criteria} also found in Rethinking (p. 223) and BDA (p. 169)
\end{comment}

\subsection{Bayesian Regularization}


The first chapter of this thesis described weight decay as a regularization technique.  Recall it left with the following full-optimization formula:

$$
F = \alpha E_W(w|A) + \beta E_D(D|w,A)
$$



This section will apply a Bayesian Approach to setting $\alpha$ and $\beta$ parameters, as told by (Mackay, 1992) \cite{mackay1992practical} and (Forsee and Hagan, 1997) \cite{foresee1997gauss}.  To begin, a probabilistic interpretation of network learning \cite{mackay1992practical} is introduced.  Under this framework, the weights of the network are represented as random variables, rather than fixed, unknown, and estimable.  By Bayes' rule:
 
$$
P(w|D,\alpha,\beta,A) = \frac{P(D|w,\beta,A) P(w|\alpha,A)}{P(D|\alpha,\beta,A)}
$$

where $D$ is the observed data and $A$ is the network architecture.\footnote{
\textcolor{darkgray}{
Note that (Mackay, 1992) defines a term $R$ as the chosen or "prior" regularizer, which represents the potential for selecting alternative control parameters for for $E_W$.  If included, Bayes' rule would then be represented  as:
$$
P(w|D,\alpha,\beta,A,R) = \frac{P(D|w,\beta,A,R) P(w|\alpha,A,R)}{P(D|\alpha,\beta,A,R)}
$$
 and subsequent formulation would be modified to represent $R$. However, this example only considers the sums of squares regularizer $E_W$ from chapter 1 and so the extended notation has been be omitted for this thesis.
 }}
Taking into assumption that the model residuals follow a normal distribution, the likelihood function \cite{foresee1997gauss}, representing the probability of the data given weights $w$, is
$$
P(D|w,\beta,A) = \frac{1}{Z_D(\beta)} e^{-\beta E_D}
$$
and by establishing the prior distribution as normal, its density is represented as:
$$
P(w|\alpha,A) = \frac{1}{Z_W(\alpha)} e^{-\alpha E_W}
$$
for which
$$
Z_D(\beta) = \left( \frac{\pi}{\beta} \right) ^{n/2} \text{ and }
Z_W(\alpha) = \left( \frac{\pi}{\alpha} \right) ^{N/2}
$$

Putting this all together, the posterior distribution can be represented as:
$$
P(w|D,\alpha,\beta,A) = \frac{\frac{1}{Z_W(\alpha)} \frac{1}{Z_D(\beta)} e^{-(\beta E_D + \alpha E_W)}}{P(D|\alpha,\beta,A)} \\
= \frac{e^{-(\beta E_D + \alpha E_W)}}{Z_F(\alpha,\beta)}
$$

The exponent in the numerator is the negative.  Thus it can be noted by the formula above that under Bayes, the optimal weights should maximize the posterior probability, which is equivalent to minimizing the full optimization formula $F = \alpha E_w(w|A) + \beta E_D(D|w,A)$ as seen in chapter 1.  For later use, the normalizing constant in the denominator is $Z_F(\alpha,\beta) = \int d^k w e^{-(\alpha E_W + \beta E_D)}$.


\textbf{Optimizing the Regularization Parameters}

By use of Bayes' rule, the posterior probability for the parameters $\alpha$ and $\beta$ is:

$$
P(\alpha, \beta | D,A) = \frac{P(D|\alpha,\beta,A) P(\alpha,\beta)}{P(D|A)}
$$



Suppose a uniform prior is chosen for ($\alpha,\beta$).  By this, the posterior distribution would be maximized by maximizing the \textit{likelihood} function.  What's more is that the likelihood function is the normalizing constant above.  With algebra, and by maintaining the previous assumptions, the likelihood function would take the form:
$$
P(D|\alpha,\beta,A) = \frac{P(D|w,\beta,A) P(w|\alpha,A)}{P(w|D,\alpha,\beta,A)}
$$
and since the heavy integration of the normalizing constant is already simplified, this equation can be computed.
$$
\frac{\frac{1}{Z_W(\alpha)} \frac{1}{Z_D(\beta)} e^{-(\beta E_D + \alpha E_W)}}{\frac{1}{Z_F(\alpha,\beta)} e^{-F(w)}} = \frac{Z_F(\alpha,\beta)}{Z_D(\beta) Z_W(\alpha)} \cdot \frac{e^{-(\beta E_D + \alpha E_W)}}{e^{-F(w)}} = \frac{Z_F(\alpha,\beta)}{Z_D(\beta) Z_W(\alpha)}
$$

The denominator is comprised of known constants $Z_D(\beta)$ and $Z_W(\alpha)$.  What is left is to determine $Z_F(\alpha,\beta)$.  Having outlined the Bayesian techniques, this thesis will not cover this decomposition.  In practice, it requires estimation by Taylor series expansion and computation of the Hessian matrix of the full optimization formula $H = \beta \nabla^ E_D + \alpha \nabla^2 E_W$ to determine the number of effective parameters in the neural network that are reducing the error function.  This decomposition can be found in (Mackay, 1992) and (Forsee and Hagan, 1997).  This technique to setting parameters $\alpha$ and $\beta$ sets the stage for the final chapter of this thesis. As will be shown, combining the complexity of neural networks with the probabilistic framework of network learning provides transparent interpretations to complex abstractions within data.


\begin{comment}
Through \textbf{margialization}, the true posterior $P(w|D,A)$ is obtained by integrating out $\alpha$ and $\beta$:
$$
P(w|D,A) = \int P(w|D,\alpha,\beta,A) P(\alpha, \beta | D,A) \text{ } d\alpha \text{ } d\beta
$$
\end{comment}
