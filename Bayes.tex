\chapter{Bayesian Statistical Methods}

Brief description, background, difference (NOT opposition) from frequentist statistics.

The frequentist approach describes probability as the relative frequency of a favorable outcome as the number of samples increases to infinity.  In Bayes, probability is the number of favorable outcomes divided by the total number of possible outcomes \cite{gelmanbayesian3}
$$
P = \frac{N_{\text{favorable outcomes}}}{N_{\text{total possible outcomes}}}
$$

\textit{(add these into the intro paragraph)}
\begin{itemize}
\tightlist
\item
Pros: usable information with little data
\item
cons: computationally intensive or impossible
\end{itemize}


\section{Fundamentals of Bayesian Inference} %-------------SECTION

\textit{(theoretical representations)}

A few things change when operating with the Bayesian framework.  For one, the goal is not to construct a model to estimate parameters as fixed values.  Rather, parameters are treated as random variables themselves, with the same properties as any other random variable - each could have a mean, a variance, minimum maximum, etc., and the distribution of that parameter reveals that information.

Exact Bayesian inference

$$
p(w|D) = \frac{p(D|w)p(w)}{p(D)} = \frac{p(D|w)p(w)}{\int_{w'} p(D|w')p(w')dw'}
$$


\subsection{From Prior to Posterior}

Selection of priors, perhaps advise on broad priors that tell little information.

Hand calculation of a Gaussian prior to compute a posterior to be used in a later section
%prior to posterior involves using MLE and the prior as we have done before in 506  Use my notes from then to compute a posterior distribution for a Gaussian prior


\subsection{Bayesian Updating}

Bayesian updating, in which the posterior is the new prior

$$
pretty ggplot display here
$$

\textit{Address computational issues like the normalizing constant, to be discussed further in the next section.}

\section{Practical Computing}



 As mentioned, the premier disadvantage of Bayesian inference is that is requires computationally intractable integrations.  In response, a world of practical approximation techniques exists, some of which will be discussed in this section.  These approximaton techniques can be shown to still have pristine results while dodging tricky or impossible integrals \cite{tipping2004bayesian}.

 Approximate Bayesian inference

$$
p(w|D) \approx \frac{p(D|w)p(w)}{Z}
$$
In which $Z$ is approximated by some sophisticated means.

 mention other approximation techniques from Rethinking like Laplace and Grid (lecture 8 notes and referenced book pages)


\subsection{Markov Chain Methods}
Used to address the problem of highly complex integral
\textit{(To be expanded into multiple subsections)}

Start with:
\cite{mcelreath2016statistical} \cite{gelmanbayesian3}
\begin{itemize}
\tightlist
\item Overview
\item Metropolis (Rosenbluth) Algorithm
\item M-H Algorithms
\item Gibbs Sampling \cite{geman1984stochastic}
\item MCMC
\item Computationally efficient versions
\end{itemize}

\textit{(Relevant examples in STAN whenever appropriate)}


\section{Bayesian Data Analysis} %-------------SECTION

\textit{(More practical representations)}

This section focuses on more tools in the Bayesian framework

\subsection{Marginalization}

Introduce marginalization to integrate unwanted parameters. (Occam's Razor things)

Construct notation by following along from 
\cite{tipping2004bayesian} and page 165 of \cite{bishop2006pattern}

Posterior Inference given (introduce the notational terms)
$$
Tipping p. 14
$$

Integrate out unwanted parameters for the \textit{predictive distribution} of a new data $t_*$:
$$
p(t_*|t,\alpha,\sigma^2) = \int p(t_*|w,\sigma^2) p(w|t,\alpha,\sigma^2) dw
$$

Where  $p(w|t,\alpha,\sigma^2)$ is the posterior distribution and $p(t_*|w,\sigma^2)$ is the likelihood of the new data.  If $w$ and $\sigma^2$ were known to be true, then the likelihood would determine the prediction for future data.  Since these are unknown, and because under Bayes parameters are random variables, the predictive distribution is a weighted average over the posterior.  Thus, the  predictive distribution can be interpreted as the expectation of the single network likelihood under the posterior $p(w|t,\alpha,\sigma^2)$. \cite{salad}


In the absence of many data points, the predictive distribution will be very sparse. \cite{tipping2004bayesian}


\textbf{Simulating the Predictive Distribution}
$$
Insert Simulationhere
$$

Ideally, to be fully Bayesian and practice the mastery of marginalization is to integrate out \textit{all} variables that are not related to the task at hand. \cite{bishop2006pattern}.

Full posterior:
$$
p(w,\alpha,\sigma^2|t) = \frac{p(t|w,\sigma^2) p(w|\alpha)p(\alpha)p(\sigma^2)}{\iiint p(t|w,\sigma^2)p(w|\alpha)p(\alpha)p(\sigma^2) \text{ } dw \text{ } d\alpha \text{ } d\sigma^2}
$$





\subsection{Predictive Accuracy Measures}
I feel I need to put Bayes-CV somewhere...

Mention of their frequentist analog (for lppd is is the log-probability score on Rethinking p. 214)

\textbf{Bayesian Cross-Validation} to get the log-pointwise predictive density
$$
lppd_{CV} = \sum_{i=1}^N \frac{1}{S} \sum_{s=1}^S logP(w_i|\theta_{-i,s})
$$

\textbf{Information Criteria} also found in Rethinking (p. 223) and BDA (p. 169)


\subsection{Bayesian Regularization}


The first chapter of this thesis described weight decay as a regularization technique.  Recall it left with the following full-optimization formula:

$$
F = \alpha E_W(w|A) + \beta E_D(D|w,A)
$$



This section will apply a Bayesian Approach to setting $\alpha$ and $\beta$ parameters, as told by (Mackay, 1992) \cite{mackay1992practical} and (Forsee and Hagan, 1997). \cite{foresee1997gauss}

The weights of the network as random variables. By Bayes' rule:
 
$$
P(w|D,\alpha,\beta,A) = \frac{P(D|w,\beta,A) P(w|\alpha,A)}{P(D|\alpha,\beta,A)}
$$

where $D$ is the observed data, and $A$ is the network architecture.\footnote{
\textcolor{darkgray}{
Note that (Mackay, 1992) defines a term $R$ as the chosen or "prior" regularizer, which represents the potential for selecting alternative control parameters for for $E_W$.  If included, Bayes' rule would then be represented  as:
$$
P(w|D,\alpha,\beta,A,R) = \frac{P(D|w,\beta,A,R) P(w|\alpha,A,R)}{P(D|\alpha,\beta,A,R)}
$$
 and subsequent formulation would be modified to represent $R$. However, this example only considers the sums of squares regularizer $E_W$ from chapter 1 and so the extended notation has been be omitted for this thesis.
 }}
Taking into assumption that the model residuals follow a normal distribution, the likelihood function \cite{foresee1997gauss}, representing the probability of the data given weights $w$, is
$$
P(D|w,\beta,A) = \frac{1}{Z_D(\beta)} e^{-\beta E_D}
$$
and by establishing the prior distribution as normal, its density is represented as:
$$
P(w|\alpha,A) = \frac{1}{Z_W(\alpha)} e^{-\alpha E_W}
$$
for which
$$
Z_D(\beta) = \left( \frac{\pi}{\beta} \right) ^{n/2} \text{ and }
Z_W(\alpha) = \left( \frac{\pi}{\alpha} \right) ^{N/2}
$$

Putting this all together, the posterior distribution can be represented as:
$$
P(w|D,\alpha,\beta,A) = \frac{\frac{1}{Z_W(\alpha)} \frac{1}{Z_D(\beta)} e^{-(\beta E_D + \alpha E_W)}}{P(D|\alpha,\beta,A)} \\
= \frac{e^{-(\beta E_D + \alpha E_W)}}{Z_F(\alpha,\beta)}
$$

Thus it can be noted by the formula above that under Bayes, the optimal weights should maximize the posterior probability, which is equivalent to minimizing the full optimization formula $F = \alpha E_w(w|A) + \beta E_D(D|w,A)$ as seen in chapter 1.  For later use, the normalizing constant in the denominator is $Z_F(\alpha,\beta) = \int d^k w e^{-(\alpha E_W + \beta E_D)}$.


\textbf{Optimizing the Regularization Parameters}

By use of Bayes' rule, the posterior probability for the parameters $\alpha$ and $\beta$ is:

$$
P(\alpha, \beta | D,A) = \frac{P(D|\alpha,\beta,A) P(\alpha,\beta)}{P(D|A)}
$$



Suppose a uniform prior is chosen for ($\alpha,\beta$).  By this, the posterior distribution would be maximized by maximizing the \textit{likelihood} function.  What's more is that the likelihood function is the normalizing constant above.  With algebra, and by maintaining the previous assumptions, the likelihood function would take the form:
$$
P(D|\alpha,\beta,A) = \frac{P(D|w,\beta,A) P(w|\alpha,A)}{P(w|D,\alpha,\beta,A)}
$$
and since the heavy integration of the normalizing constant is already simplified, this equation can be computed.
$$
\frac{\frac{1}{Z_W(\alpha)} \frac{1}{Z_D(\beta)} e^{-(\beta E_D + \alpha E_W)}}{\frac{1}{Z_F(\alpha,\beta)} e^{-F(w)}} = \frac{Z_F(\alpha,\beta)}{Z_D(\beta) Z_W(\alpha)} \cdot \frac{e^{-(\beta E_D + \alpha E_W)}}{e^{-F(w)}} = \frac{Z_F(\alpha,\beta)}{Z_D(\beta) Z_W(\alpha)}
$$

The denominator is comprised of known constants $Z_D(\beta)$ and $Z_W(\alpha)$.  What is left is to determine $Z_F(\alpha,\beta)$.  Having outlined the Bayesian techniques, this thesis will not cover this decomposition.  In practice, it requires estimation by Taylor series expansion and computation of the Hessian matrix of the full optimization formula $H = \beta \nabla^ E_D + \alpha \nabla^2 E_W$ to determine the number of effective parameters in the neural network that are reducing the error function.  This decomposition can be found in (Mackay, 1992) and (Forsee and Hagan, 1997).


\begin{comment}
Through \textbf{margialization}, the true posterior $P(w|D,A)$ is obtained by integrating out $\alpha$ and $\beta$:
$$
P(w|D,A) = \int P(w|D,\alpha,\beta,A) P(\alpha, \beta | D,A) \text{ } d\alpha \text{ } d\beta
$$
\end{comment}

\subsubsection{Earthquakes Revisited}

Introduce the brnn package

