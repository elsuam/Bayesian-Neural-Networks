\chapter{Bayesian Statistical Methods}

Often called inverse probability in its budding stages, \cite{salsburg2002lady} Bayesian Statistics is an approach to analyzing data by use a theorem developed by the Reverend Thomas Bayes; although the theorem was not published until after his death.  Bayes' Theorem (or Bayes' Rule) describes the conditional probability of an event.  The frequentist approach describes probability as the relative frequency of a favorable outcome as the number of samples increases to infinity.  In Bayes.
probability is representative of the current state of knowledge on the potential outcomes based on prior knowledge.
 \begin{comment}
In Bayes, probability is the weighted sum of favorable outcomes multiplied by the plausibility of that outcome, divided by the total number of possible outcomes \cite{mcelreath2016statistical}
$$
P = \frac{\sum_i n_{\text{ ways to produce favorable outcome i}} \cdot p_{\text{ possibility of i}} }{N_{\text{total possible outcomes}}}
$$
\end{comment}

Consider a tiny example where a coin was determined to have three potential levels of fairness assigned: $P(tails) = .75, P(tails) = .5, P(tails) = .25$.  The frequentist would flip as many times as necessary to test a hypothesis with high enough power to determine the fairness of the coin beyond reasonable doubt.  The Bayesian would assess the distributional relationship for the fairness of the coin based on prior knowledge and the likelihood of its current outcome.
Suppose after two flips the choice came up (heads,heads).  Since the set of possible outcomes is (H,H),(H,T),(T,H),(T,T), there is only one way out of four to produce (H,H).  This number is multiplied by each value of fairness to determine the potential for the true fairness value.
\begin{align*}
P(tails) &= (.75, .5, .25) \\
P(tails|H,H) &= P(H,H) \cdot \left[ P(heads) = 1- P(tails) \right] \\
&= \frac{1}{4} \cdot \frac{1}{4} = \frac{1}{16} \\
&= \frac{1}{4} \cdot \frac{1}{2} = \frac{1}{8} \\
&= \frac{1}{4} \cdot \frac{3}{4} = \frac{3}{16} \\
\end{align*}
Therefore, based on the prior knowledge of potential fairness levels, the most likely fairness is that the coin has a probability of showing tails of .25.  However, the distribution of likely fairness levels is very wide so the other options should not be ruled out.  What is important is that the parameter assignment has a distribution that still gives usable information even with very little data. As more flips are conducted, more probability density will accumulate toward the true $P(tails)$ identified by the frequentist's estimation; although under Bayes it will always be interpreted as a random variable.


\section{Fundamentals of Bayesian Inference} %-------------SECTION

%\textit{(theoretical representations)}
It is important to note that to apply statistical methods by use of Bayes does not violate any accords of frequentism and that it does not stand as an opposition to the frequentist analogue.  Bayesian methods simply offer another perspective, with some useful features in specialized cases - one of which is the ability to generate mathematically savvy results with very little data.  The goal is not to construct a model to estimate parameters as fixed values. Rather, parameters are treated as random variables themselves, with the same properties as any other random variable - each could have a mean, a variance, minimum maximum, etc., and the distribution of that parameter reveals that information.
Whereas frequentism often employes maximum likelihood modeling $p(D|\theta)$ to find the parameters $\theta$ which give the most likely estimation of the data, a Bayesian probabilistic approach finds the most likely distribution of parameters from the data $p(\theta|D)$\cite{mullachery2018bayesian}.  Bayesian inference (by Bayes' Theorem, given data set $D$) is as follows:

$$
p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)} = \frac{p(D|\theta)p(\theta)}{\int_{\theta'} p(D|\theta')p(\theta')d\theta'}
$$

This says that for each value ofind tf the parameter, count all the ways in which the data can be produced, and multiply that sum by the plausibility that the parameter is that value.  Do this for every parameter value, and divide by the sum of the product.  This gives the updated distribution of parameter values.


\subsection{From Prior to Posterior}

To determine the posterior $p(\theta|D)$ requires selection of a prior $p(\theta)$ and determination of the likelihood of the data under supposition of $\theta$, $p(D|\theta)$.  The denominator is often regarded as the \textit{normalizing constant}, which ensures the outcome is a probability between 0 and 1.  Without it, the posterior is represented as being proportional to the prior and the likelihood $p(\theta|D) \propto p(D|\theta)p(\theta)$, but not interpretable as a posterior probability.  The issue tends to be in that the normalizing constant is a hardship to calculate exactly and often impossible at all.  This is where estimation techniques come in, but more on that later.
%Selection of priors, perhaps advise on broad priors that tell little information.
Typically, priors within the same family as the likelihood (i.e. a Bernoulli likelihood and a Beta prior) have analytically helpful properties \cite{mullachery2018bayesian}. These \textit{conjugate priors}, while mathematically useful, often place undesirable assumptions on the model.  Therefore it is usually preferred not to rely on conjugate priors in order to sacrifice mathematical precision for model accuracy by approximation.
%prior to posterior involves using MLE and the prior as we have done before in 506  Use my notes from then to compute a posterior distribution for a Gaussian prior

\textbf{Bayesian Updating} is a concept in which the posterior distribution can be reused as the prior distribution when more data arrives \cite{mcelreath2016statistical}.  In the coin flip example, this would be the addition of more flips, generating a more narrow distribution centered on the most likely value of coin fairness based on the flip results.  Each time the coin is flipped, new prior probabilities for coin fairness would be generated based on the outcome from the previous flips.  This is a useful attribute because the original prior outlines some assumptions of a model, which are then used to compute the posterior, the conditional probability of the parameters in light of those assumptions and the data likelihood.  By holding on to the posterior, the model can predict more realistic expectations based on the previous assumptions and the pattern of data that the model has already seen before.

%\subsection{Bayesian Updating}




\section{Practical Computing} %-------------SECTION

As mentioned, the premier disadvantage of Bayesian inference is that is requires computationally intractable integrations.  In response, a world of practical approximation techniques exists, some of which will be discussed in this section.  These techniques use a valid distribution to approximate the posterior and can be shown to still have pristine results while dodging tricky or impossible integrals \cite{tipping2004bayesian}.
%mention other approximation techniques from Rethinking like Laplace and Grid (lecture 8 notes and referenced book pages) and lead into \textbf{Markov Chain Monte Carlo Methods}
Simpler models can be estimated by means of grid approximation \cite{mcelreath2016statistical}, in which the domain of the posterior is split up into many finite intervals and values calculated accordingly.  Quadratic Approximation is another method for approximating the posterior.  It assumes the area by the peak of the posterior distribution is approximately normally distributed, which is common \cite{mcelreath2016statistical}, and approximates that distribution because the logarithm of a normal distribution forms a quadratic function.  Both of these techniques pail in comparison to use of \textbf{Markov Chain Monte Carlo} Methods.  These methods sample the normalizing constant denominator to adequately represent the original posterior distribution to a certain degree.  In essence, a Monte Carlo method is one which uses the effect of repeated random sampling to generate results, and a Markov Chain is an iteration where the $t^{th}$ outcome is dependent upon the $(t-1)^{th}$ outcome.  MCMC is a class of algorithms based on this feature.  Some algorithms below are described for their techniques to estimating very closely a complex posterior distribution.  Unless otherwise cited, descriptions come from (McElreath, 2016 \cite{mcelreath2016statistical}) and (Gelman et. al, 2021 \cite{gelmanbayesian3}) 


%\textit{(Relevant examples in STAN whenever appropriate)}

\subsection{Metropolis Algorithm}

%-----Watch MCMC tutorials to gain a better idea of what language to use here-----

The Metropolis Algorithm is named after the first author of the published article that introduced the world to the research group's ``Fast Computing Machines'' \cite{metropolis1953equation}.  It is the foundation from which different strategies for sampling from posterior distributions arise.  The algorithm is as follows:
%although it is unclear how the work was divided. Marshall Rosenbluth and Edward Teller are co-authors, along with their wives, who performed much of the calculations.  The algorithm is as follows:

\begin{enumerate}
\tightlist
\item{Set current point $\lambda_0$ with probability $p(\lambda_0|y) > 0$}

\item{Sample a proposal $\lambda^*$ from a proposal distribution $J_t(\lambda^* | \lambda^{t-1})$ at time $t$.}

\item{The acceptence of the proposal is $A = min \left( \frac{p(\lambda^*|y)}{p(\lambda^{t-1})|y} , 1 \right) $}
\end{enumerate}

This means that the proposed move will always be accepted if it is toward a higher probability position that current.  Otherwise, it will be accepted with probability $\frac{p(\lambda*|y)}{p(\lambda^{t-1})|y}$

Works when the proposal distribution is symmetric, that is, when $J_t(\lambda^* | \lambda^{t-1}) = J_t(\lambda^{t-1} | \lambda^*)$

\subsection{Metropolis-Hastings Algorithm}

To allow for non-symmetric distributions, the Metropolis-Hastings algorithm generalizes the Metropolis algorithm.  The ratio of densities becomes:
$$
\frac{ p(\lambda^*|y) / J_t(\lambda^* | \lambda^{t-1}) }{ p(\lambda^{t-1}|y) / J_t(\lambda^{t-1} | \lambda^*) }
$$

and a minimum between that ratio and 1 is selected.

\subsection{Hamiltonian Monte Carlo}

Rather than a random walk through the posterior, HMC moves the exploration problem into a space containing a position parameter $\lambda$ and a momentum parameter $r$.  HMC has the ability to make confident, far stride and as a result converge faster than other algorithms \cite{mullachery2018bayesian}.
The parameters of HMS are step size $s$ and number of steps $L$. Sampling from a standard multivariate normal distribution, Leapfrog updates are made to determine the proposed position $\tilde{\lambda}$ and momentum $\tilde{r}$.  The proposed values are determined, through use of Laplace transform $\mathcal{L}$, by
$$
min\left( 1, \frac{exp[\mathcal{L}(\tilde{\lambda} - \frac{1}{2} \tilde{r} \cdot \tilde{r}]}
{exp[\mathcal{L}(\lambda^{m-1}) - \frac{1}{2} r^0 \cdot r^0] }
\right)
$$
and, if accepted, the new position is $\lambda^m \leftarrow \tilde{\lambda}$ and momentum $r^m \leftarrow -\tilde{r}$

\subsection{Gibbs Sampling}

Gibbs Sampling is another variant of M-H that uses clever proposals based on the criterea of alternating conditional sampling

Two-variable example: \cite{geman1984stochastic}  

$p(x,y)$ is a probability density that can be sampled more easily as conditional distributions $p(x|y)$ and $p(y|x)$.  Gibbs Samping is as follows:
\begin{enumerate}
\item{initialize $(x_0,y_0)$}
\item{sample $x_1 \sim p(x|y_0)$, then $y_1 \sim p(y|x_1)$, then $x_2 \sim p(x|y_1)$, then $y_2 \sim p(y|x_2)$}
\item{ ...}
\item{sample $x_n \sim p(x|y_{n-1})$, then $y_n \sim p(y|x_n)$}
\end{enumerate}

In higher dimensions:

\begin{itemize}
\tightlist
\item{$\lambda_j$ represents the subvector that makes up the components of $\lambda$.  For example, a parameter representing categories "blue", "red", "green", and "yellow" has four components.}
\item{At each iteration $t$, each level of $\lambda^t_j$ is sampled ($\lambda^t_{blue}$, $\lambda^t_{red}$, $\lambda^t_{green}$, $\lambda^t_{yellow}$)}
\item{ $p(\lambda_j|\lambda^{t-1}_{-j},y)$, where $\lambda^{t-1}_{-j}$ represents all components of $\lambda$ except for $\lambda_j$.}
\end{itemize}



\subsection{Variational Inference}

MCMC methods are the bets methods for approximating the posterior with samples from the exact posterior, but their Achilles' heel lies in their lack of scalability \cite{Jospin}.  Variational inference methods convert an integration problem into an optimization problem \cite{mullachery2018bayesian}.  They introduce a distribution $q(\theta;H)$ called the \textit{variational distribution} which is parameterized by $H$.  $q(\theta;H)$ must be flexible enough to provide a wide variety of posterior distributions in order to capture the posterior $p(\theta|D)$.  Closeness between the variational distribution and the true posterior is measured by Kullback-Leibler divergence \cite{blei2017variational}:
$$
KL(q||p) = E_q\left[ log\frac{q(\theta;H)}{p(\theta|D)} \right]
$$
The family of parameters $H$ is optimized to minimize $KL(q||p)$, in a similar manner that gradient descent seeks to optimize network parameters.  Much of variational inference has been omitted for this thesis, but extensive detail on these techniques can obtained from (Blai, 2017 \cite{blei2017variational}).


\section{Bayesian Data Analysis} %-------------SECTION

%\textit{(More practical representations)}

A nice feature of Bayesian statistics is the ability to incorporate prior knowledge into the model, systematically updating the relative beliefs in parameters as more data becomes available.  Even with few data points, a model can be built (although exhibit a high level of uncertainty), and from it added upon to build a better model.  Another nice property of Bayes is the ability to extract the desired distribution of interest independent of the particular parameter settings \cite{bishop2006pattern}.  Between the two of these, meaningful predictions can be extrapolated without having to employ cross-validation or bootstrapping techniques.

\subsection{Marginalization}

Bayesian inference makes use of \textit{marginalization}, a method by which nuisance parameters are integrated out rather than estimating all model parameters.
%\cite{tipping2004bayesian}.
Marginalization reduces the dimensionality by considering the sum of the estimated probabilities under each condition of the nuisance parameter.
%An essential element to Bayesian inference is \textit{marginalization}.  Rather than estimating all parameters of the model, nuisance parameters are integrated out \cite{tipping2004bayesian}.
That is, for two random variables $Y_1$ and $Y_2$ within the probability functiuon $p(y_1,y_2)$, the marginal probability of $Y_1$ would be 
$ p(y_1) = \sum_{y_2} p(y_1,y_2)$ for discrete variables and $p(y_1) = \int_{y_2} p(y_1,y_2)$ for continuous.
\cite{wackerly2014mathematical}

The same principle is applied here.  Suppose a model is determined by use of Bayes.  Its frequentist counterpart may use Ordinary Least Squares to find a set of parameters $\theta$ which minimizes its error. In Bayes, given a data set $D$, the parameters $\theta$ can be expressed by the posterior distribution $p(\theta|D,\alpha,\sigma^2)$.  Here, $\alpha$ is the decay rate seen above derived from the choice of prior, which assumes residual normailty with variation $\sigma^2$ (see Tipping, 2004 \cite{tipping2004bayesian} for derivation).
To determine the distribution of new predictions $y^*$ requires integrating the product of the posterior and the likelihood of new predictions with respect to the parameters of the model:
$$
p(y^*|D,\alpha,\sigma^2) = \int p(y^*|\theta,\sigma^2) p(\theta|D,\alpha,\sigma^2) d\theta
$$

Therefore, $p(y^*|,\alpha,\sigma^2)$ is the \textit{posterior predictive distribution} of predicted outputs $y^*$ under the circumstances set forth in the model.  If $\theta$ and $\sigma^2$ were known to be true, then the likelihood would determine the prediction for future data.  Since these are unknown, and because under Bayes parameters are random variables, the predictive distribution is a weighted average over the posterior.  Thus, the  predictive distribution can be interpreted as the expectation of the single network likelihood under the posterior \cite{salad} \cite{Jospin}.  
Setting aside the complex notation, this is simply a means of generating predictions from the parameter distribution gained by use of Bayes' Rule.  It describes the uncertainty the model has and allows for better flexibility without supplemental engineering.
With few data points to train from, the predictive distribution will be very sparse. \cite{tipping2004bayesian}

%\textbf{Simulating the Predictive Distribution}
%$$
%Insert Simulationhere
%$$

Ideally, to be fully Bayesian and practice the mastery of marginalization is to integrate out \textit{all} variables that are not directly related to the task at hand \cite{bishop2006pattern}.  If the goal is to generate a distribution for the prediction of $y^*$ given the training labels $D$, the desired distribution is $p(y^*|D)$.  For all other parameters, \textit{hyperpriors} $p(\alpha)$ and $p(\sigma^2)$ are defined. The general Bayesian predictive framework employs the full posterior \cite{tipping2004bayesian}:
$$
p(\theta,\alpha,\sigma^2|D) = \frac{p(D|\theta,\sigma^2) p(\theta|\alpha)p(\alpha)p(\sigma^2)}
{\iiint p(D|\theta,\sigma^2)p(\theta|\alpha)p(\alpha)p(\sigma^2) \text{ } d\theta \text{ } d\alpha \text{ } d\sigma^2}
$$

Bayesian inference would proceed with integrating the product of the likelihood of predictions $y^*$ by the full posterior to get $p(y^*|D)$:
$$
p(y^*|D) = \iiint p(y^*|\theta,\sigma^2) p(\theta,\alpha,\sigma^2|y)
\text{ } d\theta \text{ } d\alpha \text{ } d\sigma^2
$$

The equation above represents the distribution of model predictions based solely on the data.  It is independent of model parameters, control parameters, or noise assumptions.  However, this moves beyond the scope of this thesis.  What will be sought in future sections is the predictive distribution given the certain criteria set forth in the model; that is, for a model with specified assumptions to learning, the predictive distribution comparable to $p(y^*|D,\alpha,\sigma^2)$ marginalized over its parameters $\theta$.

%\textbf{(this is because, and show this in the later example, we are using the same architecture, same regularizer, etc.)}

%Bayesian Statistics offers a multitude of possibilities to indulge statistical models beyond estimation, such as regularizing, determining thresholds like Information Criteria, cross validating, and more.  Libraries could be filled with the studies of its potential and relative simulations on complex machine learning models.  The next section returns to discuss neural networks with applications of Bayes to select certain features.

\begin{comment}
\subsection{Predictive Accuracy Measures}
I feel I need to put Bayes-CV somewhere...

Mention of their frequentist analog (for lppd is is the log-probability score on Rethinking p. 214)

\textbf{Bayesian Cross-Validation} to get the log-pointwise predictive density
$$
lppd_{CV} = \sum_{i=1}^N \frac{1}{S} \sum_{s=1}^S logP(w_i|\theta_{-i,s})
$$

\textbf{Information Criteria} also found in Rethinking (p. 223) and BDA (p. 169)
\end{comment}

\subsection{Bayesian Regularization}


The first chapter of this thesis described weight decay as a regularization technique.  Recall it left with the following full-optimization formula:

$$
F = \alpha E_W(W|\mathcal{A}) + \beta E_D(D|W,\mathcal{A})
$$



This section will apply a Bayesian Approach to setting $\alpha$ and $\beta$ parameters, as told by (Mackay, 1992) \cite{mackay1992practical} and (Forsee and Hagan, 1997) \cite{foresee1997gauss}.  To begin, a probabilistic interpretation of network learning \cite{mackay1992practical} is introduced.  Under this framework, the weights of the network are represented as random variables, rather than fixed, unknown, and estimable.  By Bayes' rule:
 
$$
P(W|D,\alpha,\beta,\mathcal{A}) = \frac{P(D|W,\beta,\mathcal{A}) P(W|\alpha,\mathcal{A})}{P(D|\alpha,\beta,\mathcal{A})}
$$

where $D$ is the observed data and $\mathcal{A}$ is the network architecture.\footnote{
\textcolor{darkgray}{
Note that (Mackay, 1992) defines a term $\mathcal{R}$ as the chosen or "prior" regularizer, which represents the potential for selecting alternative control parameters for for $E_W$.  If included, Bayes' rule would then be represented  as:
$$
P(w|D,\alpha,\beta,\mathcal{A},\mathcal{R}) = \frac{P(D|w,\beta,\mathcal{A},\mathcal{R}) P(w|\alpha,\mathcal{A},\mathcal{R})}{P(D|\alpha,\beta,\mathcal{A},\mathcal{R})}
$$
 and subsequent formulation would be modified to represent $\mathcal{R}$. However, this example only considers the sums of squares regularizer $E_W$ from chapter 1 and so the extended notation has been be omitted for this thesis.
 }}
Taking into assumption that the model residuals follow a normal distribution, the likelihood function \cite{foresee1997gauss}, representing the probability of the data given weights $W$, is
$$
P(D|W,\beta,\mathcal{A}) = \frac{1}{Z_D(\beta)} e^{-\beta E_D}
$$
and by establishing the prior distribution as normal, its density is represented as:
$$
P(W|\alpha,\mathcal{A}) = \frac{1}{Z_W(\alpha)} e^{-\alpha E_W}
$$
for which
$$
Z_D(\beta) = \left( \frac{\pi}{\beta} \right) ^{n/2} \text{ and }
Z_W(\alpha) = \left( \frac{\pi}{\alpha} \right) ^{N/2}
$$

Putting this all together, the posterior distribution can be represented as:
$$
P(W|D,\alpha,\beta,\mathcal{A}) = \frac{\frac{1}{Z_W(\alpha)} \frac{1}{Z_D(\beta)} e^{-(\beta E_D + \alpha E_W)}}{P(D|\alpha,\beta,\mathcal{A})} \\
= \frac{e^{-(\beta E_D + \alpha E_W)}}{Z_F(\alpha,\beta)}
$$

The exponent in the numerator is the negative of $F$.  Thus it can be noted by the formula above that under Bayes, the optimal weights should maximize the posterior probability, which is equivalent to minimizing the full optimization formula $F = \alpha E_w(W|\mathcal{A}) + \beta E_D(D|W,\mathcal{A})$ as seen in chapter 1.
%For later use, the normalizing constant in the denominator is $Z_F(\alpha,\beta) = \int d_k w e^{-(\alpha E_W + \beta E_D)}$ where $d_k$ is the data point pertaining to the $k^{th}$ parameter value.


\textbf{Optimizing the Regularization Parameters}

By use of Bayes' rule, the posterior probability for the parameters $\alpha$ and $\beta$ is:

$$
P(\alpha, \beta | D,\mathcal{A}) = \frac{P(D|\alpha,\beta,\mathcal{A}) P(\alpha,\beta)}{P(D|\mathcal{A})}
$$



Suppose a uniform prior is chosen for ($\alpha,\beta$).  By this, the posterior distribution would be maximized by maximizing the \textit{likelihood} function.  What's more is that the likelihood function is the normalizing constant above.  With algebra, and by maintaining the previous assumptions, the likelihood function would take the form:
$$
P(D|\alpha,\beta,\mathcal{A}) = \frac{P(D|W,\beta,\mathcal{A}) P(W|\alpha,\mathcal{A})}{P(W|D,\alpha,\beta,\mathcal{A})}
$$
and since the heavy integration of the normalizing constant is already simplified, this equation can be computed.
$$
\frac{\frac{1}{Z_W(\alpha)} \frac{1}{Z_D(\beta)} e^{-(\beta E_D + \alpha E_W)}}{\frac{1}{Z_F(\alpha,\beta)} e^{-F(W)}} = \frac{Z_F(\alpha,\beta)}{Z_D(\beta) Z_W(\alpha)} \cdot \frac{e^{-(\beta E_D + \alpha E_W)}}{e^{-F(W)}} = \frac{Z_F(\alpha,\beta)}{Z_D(\beta) Z_W(\alpha)}
$$

The denominator is comprised of known constants $Z_D(\beta)$ and $Z_W(\alpha)$.  What is left is to determine $Z_F(\alpha,\beta)$.  Having outlined the Bayesian techniques, this thesis will not cover this decomposition.  In practice, it requires estimation by Taylor series expansion and computation of the Hessian matrix of the full optimization formula $H = \beta \nabla^ E_D + \alpha \nabla^2 E_W$ to determine the number of effective parameters in the neural network that are reducing the error function.  This decomposition can be found in (Mackay, 1992) and (Forsee and Hagan, 1997).  This technique to setting parameters $\alpha$ and $\beta$ sets the stage for the final chapter of this thesis. As will be shown, combining the complexity of neural networks with the probabilistic framework of network learning provides transparent interpretations to complex abstractions within data.


\begin{comment}
Through \textbf{margialization}, the true posterior $P(w|D,A)$ is obtained by integrating out $\alpha$ and $\beta$:
$$
P(w|D,A) = \int P(w|D,\alpha,\beta,A) P(\alpha, \beta | D,A) \text{ } d\alpha \text{ } d\beta
$$
\end{comment}
