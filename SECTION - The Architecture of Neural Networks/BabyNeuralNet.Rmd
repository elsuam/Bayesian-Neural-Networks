---
title: "Analyzing the Structure of a Neural Network"
subtitle: "An Applied Approach with Theoretical Backing"
author: "Samuel Richards"
output:
  pdf_document:
    keep_tex: TRUE
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: no
bibliography: BabyNNref.bib
---

<!--Use this file for a separate project; modify it for Thesis when it is in Overleaf -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = 'BabyNeuralNet_files/')
library(tidyverse)
library(gridExtra)
```

# Load Data

```{r}
planar_dataset <- function(){   #code to manually replicate the planar data set (run lines 6-34 together)
  set.seed(1)
  m <- 400
  N <- m/2
  D <- 2
  X <- matrix(0, nrow = m, ncol = D)
  Y <- matrix(0, nrow = m, ncol = 1)
  a <- 4
  
  for(j in 0:1){
    ix <- seq((N*j)+1, N*(j+1))
    t <- seq(j*3.12,(j+1)*3.12,length.out = N) + rnorm(N, sd = 0.2)
    r <- a*sin(4*t) + rnorm(N, sd = 0.2)
    X[ix,1] <- r*sin(t)
    X[ix,2] <- r*cos(t)
    Y[ix,] <- j
  }
  
  d <- as.data.frame(cbind(X, Y))
  names(d) <- c('X1','X2','Y')
  d
}


df <- planar_dataset()


ggplot(df, aes(x = X1, y = X2, color = factor(Y))) +
  geom_point()
```


# Preprocessing

## Train/Test split

***After I make this my own, change the data set and run a 70/30 split***
```{r}
set.seed(69)

df <- df[sample(nrow(df)), ]  #shuffle the data
head(df)

train_test_split <- 0.7 * nrow(df)  #70% of data into our train set and the remaining 30% in our test set

train <- df[1:train_test_split,]  #extracting first (shuffled) 70% of rows into a dataset
head(train)

test <- df[(train_test_split+1): nrow(df),] #extracting remaining (shuffled) 20% of rows into test datset
head(test)
```


## Assess for Class Imbalance

```{r}
#plots and numeric proportions of classes to assess for severe imbalance
#for train:
trainplot <- ggplot(train, aes(x = Y, fill = factor(Y))) +
  geom_bar(aes(y = (..count..)/sum(..count..)))

fortrain <- round(prop.table(table(train$Y)), 3) 

#for test:
testplot <- ggplot(test, aes(x = Y, fill = factor(Y))) +
  geom_bar(aes(y = (..count..)/sum(..count..)))

fortest <- round(prop.table(table(test$Y)), 3)

grid.arrange(trainplot + labs(title = "Training data", fill="classification", y = "Proportion of Each Class"), testplot  + labs(title = "Test data", fill="classification", y = "Proportion of Each Class"), ncol = 2)
```

The relevant proportions of each class for the training data is `r fortrain`.  
The relevant proportions of each class for the test data is `r fortest`.


## Standardize

We normalize the data to ensure that our activation functions take both positive and negative values.  If we have features that are all positive or all negative, the learning process will zig-zag toward extrema very wildly and make learning harder.  By standardizing, we not only avoid this but also ensure that the distributions of our weights and bias are the same (and therefore they will learn at nearly the same rate), making for faster convergence.

```{r}
#standardizing the data
##note that we are standardizing the input values, not the output (Y) values

train_x <- scale(train[, c(1:2)])   #standardize all rows in the second two columns (X1 and X2)
train_y <- train$Y
dim(train_y) <- c(length(train_y), 1) # add extra dimension to vector

test_x <- scale(test[, c(1:2)])
test_y <- test$Y
dim(test_y) <- c(length(test_y), 1) # add extra dimension to vector
```


## Convert to Matrices

Converting to matrices speeds up the calculation.

```{r}
#converting dataframes to matrices, and taking the transpose, for matrix multiplications
train_x <- as.matrix(train_x, byrow=TRUE)
train_x <- t(train_x)                     #transpose
train_y <- as.matrix(train_y, byrow=TRUE)
train_y <- t(train_y)

test_x <- as.matrix(test_x, byrow=TRUE)
test_x <- t(test_x)
test_y <- as.matrix(test_y, byrow=TRUE)
test_y <- t(test_y)


#shapes of matrices (to confirm with HTML reference page)
dim(train_x)
dim(train_y)
dim(test_x)
dim(test_y)
```



# Building the Network

## Layer Sizes
(Write about layer sizes here)


### Function in R:

(Explain this function in more detail)

```{r}
LayerSize <- function(x, y, hidden, train=TRUE) {
  n_x <- dim(x)[1]  #returns the value of 2 based on the dimensions (train_x is (2*280))
  n_h <- hidden     # number of neurons in the hidden layer
  n_y <- dim(y)[1]  #returns the value of 1 (train_y is (1*280))
  
  size <- list("Input layer size" = n_x,
               "Hidden layer size" = n_h,
               "Output layer size" = n_y)
  
  return(size)
}
layer_size <- LayerSize(train_x, train_y, hidden = 4)
layer_size #returns a list of each layers' sizes
```
- The input layer has `r layer_size[[1]]` neurons.
- The hidden layer has `r layer_size[[2]]` neurons.
- The output layer has `r layer_size[[3]]` neurons.


## Initialize parameters

We will select our starting parameters from a uniform distribution $Unif(0,1)$.  
Because this network has three layers, we will have two sets of parameters (a matrix of weights and a matrix of biases for both the input layer and for the hidden layer).  

- The shape of the weight matrix for the input layer is $(N_{hidden},N_{input})$
- The shape of the bias matrix for the input layer is $(N_{hidden},1)$
- The shape of the weight matrix for the hidden layer is $(N_{output},N_{hidden})$
- The shape of the bias matrix for the hidden layer is $(N_{output},1)$


### Function in R:

(Explain this function in more detail)

```{r}
initializeParameters <- function(X, layer_sizes){

  m <- dim(data.matrix(X))[1]
  
  n_x <- layer_sizes[[1]]
  n_h <- layer_sizes[[2]]
  n_y <- layer_sizes[[3]]
  
  W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
  b1 <- matrix(rep(0, n_h), nrow = n_h)
  W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
  b2 <- matrix(rep(0, n_y), nrow = n_y)
  
  params <- list("Initial weights layer 1 (W1)" = W1,
                 "Initial biases layer 1 (b1)" = b1, 
                 "Initial weights hidden layer (W2)" = W2,
                 "Initial biases hidden layer (b2)" = b2)
  
  return (params)
}

init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
```

- The shape of the weight matrix for the input layer is (`r dim(init_params[[1]])`)
- The shape of the bias matrix for the input layer is (`r dim(init_params[[2]])`)
- The shape of the weight matrix for the hidden layer is (`r dim(init_params[[3]])`)
- The shape of the bias matrix for the hidden layer is (`r dim(init_params[[4]])`)

## Activation Functions

(Write about activation functions and discuss their differences.)

```{r, figures-side, fig.show="hold", out.width="50%"}
#creating and plotting the sigmoid function
sigmoid <- function(x){
  return(1 / (1 + exp(-x)))
}

relu <- function(x){
  ifelse (x<0,0,x) 
}

lrelu <- function(x){
  ifelse (x<0,0.1*x,x) 
}

f <- seq(-8,8, length=1000)

plot(f,sigmoid(f), main="Sigmoid", xlab = " ", ylab = " ")
plot(f,tanh(f), main="Hyperbolic Tangent", xlab = " ", ylab = " ")
plot(f,relu(f), main="ReLU", xlab = " ", ylab = " ")
plot(f,lrelu(f), main="Leaky ReLU", xlab = " ", ylab = " ")
```


## Forward Propagation

(write about this step)

### Function in R:

(refine the details of this function)

matrix multiplication dimensions:
`W1 %*% X` dimensions: `(4,2)` x `(2,320)` = `(4,320)`
`W2 %*% A1` dimensions: `(1,4)` x `(4,320)` = `(1,320)`

The `forward` function uses the input training data, the weights and biases (starting with the initial ones calculated above), and the size of the layers to propagate forward through the network.  
After calling the relevant parameters, the function first repeats the biases to be able to add that bias to each component of the preceding multiplication.  The function multiplies the matrix of input training values by the matrix of weights to pass through the first layer of the network, adds the bias to each value of the resulting matrix, and passes that through the activation function.  It then multiplies the resulting matrix by the matrix of weights in the hidden layer, finalizing by passing it through another activation function.  

```{r}
forward <- function(X, params, layer_sizes){
#function of the input matrix (X), the list of parameters, and the list of layer sizes
  
  m <- dim(X)[2]  #second dimension of shape x (280)
  n_h <- layer_sizes[[2]]
  n_y <- layer_sizes[[3]]
  
  W1 <- params[[1]]
  b1 <- params[[2]]
  W2 <- params[[3]]
  b2 <- params[[4]]
  
  b1_new <- matrix(rep(b1, m), nrow = n_h)  #repeat b1 to be able to add the bias to each component of the matrix                                               multiplication step (weight*input)
  b2_new <- matrix(rep(b2, m), nrow = n_y)  #repeat b1 to be able to add the bias to each component of the matrix                                               multiplication step (weight*hidden)
  
  Z1 <- W1 %*% X + b1_new  #(input to hidden) matrix multiplication of weight and input matrix
  A1 <- sigmoid(Z1) #activation function for the previous line
  Z2 <- W2 %*% A1 + b2_new #(hidden to output) matrix multiplication of weights and hidden layer matrix
  A2 <- sigmoid(Z2) #activation function for the previous line

  
  cache <- list("Z1" = Z1,
                "A1" = A1, 
                "Z2" = Z2,
                "A2" = A2)
  
  return (cache)
}

#matrix multiplications explained:
# W1 %*% X dimensions: (4,2)*(2,320) = (4,320)
# W2 %*% A1 dimensions: (1,4)*(4,320) = (1,320)
#biases are repeated so to be added to each component of the respective matrix

fwd_prop <- forward(train_x, init_params, layer_size)
lapply(fwd_prop, function(x) dim(x))
```
- Output matrix from the input layer: (`r dim(fwd_prop[[1]])`)
- Activation function: (`r dim(fwd_prop[[2]])`)
- Output matrix from the hidden layer: (`r dim(fwd_prop[[3]])`)
- Activation function (end result per iteration ): (`r dim(fwd_prop[[4]])`)

All four calculations are stored to be used in the backpropagation algorithm, but first the final calculation is used to compute cost.


## Computing Cost

(Change this completely to utilize the correct loss function)

uses the Mean Squared Error loss function to compare true values with predicted output

$$
C = \frac{1}{n} \sum_{i=1}^N (a^{(L)}_i - y_i)
$$
Where $a^{(L)}_i$ is the outcome of a single iteration of forward propagation and $y_i$ is the true observation to compare it to.

### Function in R

(Change this to reflect the correct loss function)  
(Explain this function in more detail)

```{r}
costComp <- function(X, y, cache) {
  m <- dim(X)[2]
  A2 <- cache[[4]]  #y-hat
  logprobs <- (log(A2) * y) + (log(1-A2) * (1-y))
  cost <- -sum(logprobs/m)
  return (cost)
}
cost <- costComp(train_x, train_y, fwd_prop)
cost
```


## Backpropagation

**Backpropagation** is the algorithm that computes the gradient for high-dimensional derivatives for the process of **gradient descent**, which helps us reduce the cost function.  For a standard neural network, it is composed of the partial derivatives of the cost function with respect to each weight in each layer of the network, and each bias in each layer of the network.

$$
\nabla{C} =
\begin{bmatrix}
\frac{\partial{C}}{\partial{w_1^{(1)}}} & \frac{\partial{C}}{\partial{w_2^{(1)}}} & \cdots & 
\frac{\partial{C}}{\partial{w_i^{(1)}}} \\
\frac{\partial{C}}{\partial{b_1^{(1)}}} & \frac{\partial{C}}{\partial{b_2^{(1)}}} & \cdots & 
\frac{\partial{C}}{\partial{b_i^{(1)}}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial{C}}{\partial{w_1^{(l)}}} & \frac{\partial{C}}{\partial{w_2^{(l)}}} & \cdots & 
\frac{\partial{C}}{\partial{w_i^{(l)}}} \\
\frac{\partial{C}}{\partial{b_1^{(l)}}} & \frac{\partial{C}}{\partial{b_2^{(l)}}} & \cdots & 
\frac{\partial{C}}{\partial{b_i^{(l)}}} \\
\end{bmatrix}
$$
Here, the superscript $^{(l)}$ denotes the layer of the network and the subscript $_i$ denotes a specific parameter within that specific layer.  Since we have two layers, $^{(2)}$ indicates the hidden layer and $^{(1)}$ the input layer.

### Theoretical Formulation

  The backpropagation algorithm is catered to a specific loss function.  In this example, the loss function is defined above as *Mean Squared Error*.  This function takes the difference between the output from the network and the specified desired output for each observation.  To understand how each parameter that makes up the output must be adjusted, the algorithm must differentiate every part of it.  This is done by use of the chain rule in calculus.  Ultimately, two parameter differentiations must be computed for each observation in each layer - a *weight* and a *bias*, as illustrated in the matrix above.  
  
  The chain rule is applied when tracing the lineage of influence on the network's output.  In tracing the architecture of the network backwards (or "back propagating"), the output is found to be influenced by:
  
  - an activation function in between the hidden layer and the output, which is influenced by the resulting computation of the hidden layer *weight*, *bias*, and
  - an activation function from the previous layer, which is which is influenced by the resulting computation of the previous layer *weight*, *bias*, and activation of earlier layers.  
  
  Because our network only has an input and hidden layer, this "previous" layer is the input layer, and the "activation of earlier layers" is our input x-value.
  
#### Partial derivatives with respect to weight

The amount we must adjust the weight in the *hidden* layer is represented by:
$$
\begin{eqnarray}
\frac{\partial{C}}{\partial{w_i^{(2)}}}    &=&  \dfrac{\partial{z_i^{(2)}}}{\partial{w_i^{(2)}}}
     \dfrac{\partial{a_i^{(2)}}}{\partial{z_i^{(2)}}}
     \dfrac{\partial{C}}{\partial{a_i^{(2)}}} \\
     
 &=& a_i^{(1)} \sigma'(z_i^{(2)}) 2(a_i^{(2)}-y_i) \\
\end{eqnarray}
$$

Where $a_n^{(1)}$ is the activation for each observation in the first (input) layer, $\sigma'(z_i^{(2)})$ is the derivative of the sigmoid activation function with respect to each observation in the second (hidden) layer, $a_i^{(2)}$ is the activation for each observation in the hidden layer, and $y_i$ is the vector of actual training values this network hopes to achieve.


The amount we must adjust the weight in the *input* layer is represented by:
$$
\begin{eqnarray}
\frac{\partial{C}}{\partial{w_i^{(1)}}}    &=& \dfrac{\partial{z_i^{(1)}}}{\partial{w_i^{(1)}}} \dfrac{\partial{a_i^{(1)}}}{\partial{z_i^{(1)}}}  \dfrac{\partial{z_i^{(2)}}}{\partial{a_i^{(1)}}}
     \dfrac{\partial{a_i^{(2)}}}{\partial{z_i^{(2)}}}
     \dfrac{\partial{C}}{\partial{a_i^{(2)}}} \\
     
&=& x_i \sigma'(z_i^{(1)}) w_i^{(2)} \sigma'(z_i^{(2)}) 2(a_i^{(2)}-y_i) \\

\end{eqnarray}
$$
where $x_i$ is the input from our training data, $\sigma'(z_i^{(1)})$ is the derivative of the sigmoid activation function with respect to each observation in the input layer, $w_i^{(2)}$ is the vector of weights in the hidden layer, and all else is the same as is in the previous equation.


#### Partial derivatives with respect to bias

The amount we must adjust the bias in the *hidden* layer is represented by:
$$
\frac{\partial{C}}{\partial{b_i^{(2)}}}  =  \dfrac{\partial{z_i^{(2)}}}{\partial{b_i^{(2)}}}
     \dfrac{\partial{a_i^{(2)}}}{\partial{z_i^{(2)}}}
     \dfrac{\partial{C}}{\partial{a_i^{(2)}}} \\
$$

Because the bias is a constant, that is $\dfrac{\partial{z_i^{(l)}}}{\partial{b_i^{(l)}}} = 1$, our chain-rule formula simplifies to the following:
$$
\frac{\partial{C}}{\partial{b_i^{(2)}}} = 1 \cdot \sigma'(z_i^{(2)}) 2(a_i^{(2)}-y_i) \\
$$

The amount we must adjust the bias in the *input* layer is represented by:
$$
\begin{eqnarray}
\frac{\partial{C}}{\partial{b_i^{(1)}}}    &=& \dfrac{\partial{z_i^{(1)}}}{\partial{b_i^{(1)}}} \dfrac{\partial{a_i^{(1)}}}{\partial{z_i^{(1)}}}  \dfrac{\partial{z_i^{(2)}}}{\partial{a_i^{(1)}}}
     \dfrac{\partial{a_i^{(2)}}}{\partial{z_i^{(2)}}}
     \dfrac{\partial{C}}{\partial{a_i^{(2)}}} \\
     
&=& 1 \cdot \sigma'(z_i^{(1)}) w_i^{(2)} \sigma'(z_i^{(2)}) 2(a_i^{(2)}-y_i) \\

\end{eqnarray}
$$


### Function in R:

Below is the function that computes backpropagation for the *Mean Square Error* loss function.

- `dW1`contains the necessary adjustment needed to the weights in the input layer $\frac{\partial{C}}{\partial{w_i^{(1)}}}$
- `dW2`contains the necessary adjustment needed to the weights in the hidden layer $\frac{\partial{C}}{\partial{w_i^{(2)}}}$
- `dB1`contains contains the necessary adjustment needed to the biases in the input layer
$\frac{\partial{C}}{\partial{b_i^{(1)}}}$
- `dB2`contains contains the necessary adjustment needed to the biases in the hidden layer
$\frac{\partial{C}}{\partial{b_i^{(2)}}}$

```{r}
dxsigmoid <- function(x) {
  (1/(1+exp(-x))) * (1 - (1/(1+exp(-x))))
}

#not correct derivative below; used for testing stuff
#dxsigmoid <- function(x) {
#  x * (1.0 - x)
#}
####

backp <- function(x, y, cache, params,layer_sizes){

  A2 <- cache[[4]]
  A1 <- cache[[2]]
  Z2 <- cache[[3]]
  W2 <- params[[3]]
  Z1 <- cache[[1]]
  
    m <- dim(x)[2]
  
  n_x <- layer_sizes[[1]]
  n_h <- layer_sizes[[2]]
  n_y <- layer_sizes[[3]]
  
  dW2 <- A1 %*% t(dxsigmoid(Z2) * 2*(A2-y)) #CRUCIAL INFO - look at matrix multipliers
  
  dW1 <- t(W2) %*% (dxsigmoid(Z2) * 2*(A2-y))
  dW1 <- dW1*dxsigmoid(Z1)
  dW1 <- dW1 %*% t(x)
  
  dB2 <- matrix(dxsigmoid(Z2) %*% t(2*(A2-y)), nrow = n_y)
#  dB2 <- matrix(rep(dB2, m), nrow = n_y) 
  
  dB1 <- matrix((W2 %*% dxsigmoid(Z1) %*% t(dxsigmoid(Z2) * 2*(A2-y))) , ncol = n_h)
#  dB1 <- matrix(rep(dB1, m), nrow = n_h)
  
    gradient <- list("dW1" = dW1,
                     "dW2" = dW2,
                     "dB1" = dB1,
                     "dB2" = dB2)
  
  return(gradient)
    
}

back <- backp(train_x, train_y, fwd_prop, init_params,layer_size)
back
```


```{r include=FALSE, results= 'asis'}
write_matex <- function(x) {
  begin <- "$$\\begin{bmatrix}"
  end <- "\\end{bmatrix}$$"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  writeLines(c(begin, X, end))
}

aa <- write_matex(back[[1]])
bb <- write_matex(back[[2]])
cc <- write_matex(back[[3]])
dd <- write_matex(back[[4]])
```


- The shape of the weight matrix for the input layer is (`r dim(back[[1]])`)
- The shape of the bias matrix for the input layer is (`r dim(back[[2]])`)
- The shape of the weight matrix for the hidden layer is (`r dim(back[[3]])`)
- The shape of the bias matrix for the hidden layer is (`r dim(back[[4]])`)



## Updating parameters

Parameters are updated by applying the gradient fro the previous step to the respective weights and biases.  Since the aim is to reduce the cost, we *subtract* the gradient terms.  Additionally, a "step size" is multiplied to the gradient terms to moderate how fast the machine learns: too large a step and the machine can bounce in and out of the desired minimum; too small and it will be very computationally costly to reach the desired point. 

```{r}
updateParameters <- function(gradient, params, stepsize){
  
  W1 <- params[[1]]
  b1 <- params[[2]]
  W2 <- params[[3]]
  b2 <- params[[4]]
  
  dW1 <- gradient[[1]]
  db1 <- gradient[[2]]
  dW2 <- gradient[[3]]
  db2 <- gradient[[4]]
  
  W1 <- W1 - stepsize * dW1
  b1 <- b1 - stepsize * db1
  W2 <- W2 - stepsize * dW2
  b2 <- b2 - stepsize * db2
  
  updated_params <- list("W1" = W1,
                         "b1" = b1,
                         "W2" = W2,
                         "b2" = b2)
  
  return (updated_params)
}

update_params <- updateParameters(back, init_params, stepsize = 0.01)
#lapply(update_params, function(x) dim(x))
```

Initial matrix for the weights in layer 1:
```{r echo=FALSE, results= 'asis'}
write_matex(init_params[[1]])
```

updated matrix for the weights in layer 1:
```{r echo=FALSE, results= 'asis'}
write_matex(update_params[[1]])
```

If we subtract the two and multiply by 100 to reverse the step size, we can see that the results are exactly identical to the results for the weights in the first layer from the backpropagation step - so we're still on track!
```{r}
(init_params[[1]] - update_params[[1]])*100
back[[1]]
```


# Training the Model




# Conclusion

## References

[R Views page part 1](https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/)  
[R Views page part 2](https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/)  
[3Blue1Brown lesson](https://www.3blue1brown.com/lessons/backpropagation-calculus)  
@nielsen
@Rumelhart1986LearningRB


<!-- In case I'm wrong and need my notation back:


**Partial derivatives with respect to weight**

$$
\begin{eqnarray}
\frac{\partial{C}}{\partial{w_n^{(l)}}}    &=&  \dfrac{\partial{z_n^{(l)}}}{\partial{w_n^{(l)}}}
     \dfrac{\partial{a_n^{(l)}}}{\partial{z_n^{(l)}}}
     \dfrac{\partial{C}}{\partial{a_n^{(l)}}} \\
     
 &=& \frac{1}{m}dz_n^{(l)}a_n^{(l-1)}
\end{eqnarray}
$$
Where $d{z_n^{(l)}}$ is the input to the layer and $a^{(l-1)}$ is the previous layer's activation.


**Partial derivatives with respect to bias**

$$
\begin{eqnarray}
\dfrac{\partial{C}}{\partial{b_n^{(l)}}}   &=& \dfrac{\partial{z_n^{(l)}}}{\partial{b_n^{(l)}}}
     \dfrac{\partial{a_n^{(l)}}}{\partial{z_n^{(l)}}}
     \dfrac{\partial{C}}{\partial{a_n^{(l)}}} \\


 &=& \frac{1}{m}\sum_{n=1}^m dz_n^{(l)}
\end{eqnarray}
$$
Where $d{z_n^{(l)_i}}$ is 

**partial derivatives with respect to previous level activation:**

$$
\begin{eqnarray}
\dfrac{\partial{C}}{\partial{a_n^{(l-1)}}}  &=& \dfrac{\partial{z_n^{(l)}}}{\partial{a_n^{(l-1)}}}
     \dfrac{\partial{a_n^{(l)}}}{\partial{z_n^{(l)}}}
     \dfrac{\partial{C}}{\partial{a_n^{(l)}}} \\
     
&=& w_n^{(l)} dz_n^{(l)}
\end{eqnarray}
$$
Where $w_n^{(l)}$ is the weight of the current layer.

-->





```{r eval=FALSE, include=FALSE}

#--to test the current backprop function---

A_2 <- fwd_prop[[4]]
A_1 <- fwd_prop[[2]]
Z_2 <- fwd_prop[[3]]
W_2 <- init_params[[3]]
Z_1 <- fwd_prop[[1]]

dim(A_1)
dim(A_2)
dim(Z_2)
dim(W_2)
dim(Z_1)

  dB_1 <- (W_2 %*% dxsigmoid(Z_1) %*% t(dxsigmoid(Z_2) * 2*(A_2-train_y))) 
dim(dB_1)

dim(train_x)

  dW_2 <- (dxsigmoid(Z_2) * 2*(A_2-train_y))  %*%  t(A_1) #CRUCIAL INFO - look at matrix multipliers
dim(dW_2) #solved


  dW_1 <- t(W_2) %*% (dxsigmoid(Z_2) * 2*(A_2-train_y))
dim(dW_1)
  dW_1 <- dW_1*dxsigmoid(A_1)
  dW_1 <- dW_1 %*% t(train_x)
```









```{r eval=FALSE, include=FALSE}

#--original backprop function with my work and confucion below---2/22/23

backward <- function(X, y, cache, params, layer_sizes){
  
  m <- dim(X)[2]
  
  n_x <- layer_sizes[[1]]
  n_h <- layer_sizes[[2]]
  n_y <- layer_sizes[[3]]
  
  A2 <- cache[[4]]
  A1 <- cache[[2]]
  W2 <- params[[3]]
  
  dZ2 <- A2 - y                               #y-hat (outcome) minus y (actual) = dZ of output layer
  dW2 <- 1/m * (dZ2 %*% t(A1))                #differentiate the loss function with respect to the weight of the                                                  output layer
  db2 <- matrix(1/m * sum(dZ2), nrow = n_y)   #differentiate the loss function with respect to the bias of the                                                    output layer
  db2_new <- matrix(rep(db2, m), nrow = n_y)  #to apply the biases to each part of the matrix multiplication
  
  dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)         #
  
  dW1 <- 1/m * (dZ1 %*% t(X))                 #differentiate the loss function with respect to the weight of the                                                  hidden layer
  db1 <- matrix(1/m * sum(dZ1), nrow = n_h)
  db1_new <- matrix(rep(db1, m), nrow = n_h)

  gradient <- list("dW1" = dW1, 
                "db1" = db1,
                "dW2" = dW2,
                "db2" = db2)
  
  return(gradient)
}

backprop <- backward(train_x, train_y, fwd_prop, init_params, layer_size)
#lapply(backprop, function(x) dim(x))

x <- fwd_prop[[1]]
der1 <- eval(dxsigmoid)
x <- fwd_prop[[3]]
der2 <- eval(dxsigmoid)

dim(fwd_prop[[2]])    #A^1
#dim(fwd_prop[[1]])    #Z_1
dim(eval(dxsigmoid))  #sigoid'(Z_1)
#dim(fwd_prop[[4]])    #A^2
dim(train_y)          #y

#tutorial equivalents:---A^1--                              ----------dz2---------
something <- t(fwd_prop[[2]]) %*% eval(dxsigmoid) %*% t(2*(fwd_prop[[4]] - train_y))   #what d0es this mean?

something2 <- (fwd_prop[[4]] - train_y) %*%  t(fwd_prop[[2]]) 
#this means the function for dW2 in the tutorial does not use the derivative of the cost function (??)
# nor does is multiply by 2 as in the notes does

dim(something)
dim(backprop[[3]])
m <- dim(train_x)[2]

something2/m
backprop[[3]]   #dW2 from the function


dim(init_params[[3]])
dim(eval(dxsigmoid))  #sigmoid'(Z_1)
dim(train_y)          #y

somethingelse <- (eval(dxsigmoid) %*% t(2*(fwd_prop[[4]] - train_y)) %*% init_params[[3]])/m  #dZ1 equivalent
dim(somethingelse)


dZ1 <- (t(init_params[[3]]) %*% (fwd_prop[[4]] - train_y)) * (1 - fwd_prop[[2]]^2)
dim(dZ1)  

dim(init_params[[3]])
dim(train_y)  
dim(der1)
dim(der2)
dim(fwd_prop[[1]])

dZ331 <- (t(init_params[[3]]) %*% (fwd_prop[[4]] - train_y)) %*% t(eval(dxsigmoid)) %*% (fwd_prop[[1]])
dim(dZ331)

mean(dZ1)*1000
mean(dZ331)*1000

dW1 <- 1/m * (dZ1 %*% t(train_x))

dim(dW1)

  
  
  
  
x <- fwd_prop[[1]]
der1 <- eval(dxsigmoid)
x <- fwd_prop[[3]]
der2 <- eval(dxsigmoid)

dim(der1)
dim(init_params[[3]])
dim(der2)
dim(fwd_prop[[4]]-train_y)

h <- der1*sum(t(init_params[[3]]) %*% der2 %*% t(2*(fwd_prop[[4]]-train_y)) )
dim(h)

h <- der1*sum(t(init_params[[3]]) %*% der2 %*% t(2*(fwd_prop[[4]]-train_y)) )
dim(h)

dim(init_params[[1]])
dim(der1)
dim(fwd_prop[[2]]-fwd_prop[[1]])
i <- train_x*sum(t(init_params[[1]]) %*% der1 %*% t(2*(fwd_prop[[2]]-fwd_prop[[1]])))
dim(i)

wut <- h %*% t(i)
wut


```

